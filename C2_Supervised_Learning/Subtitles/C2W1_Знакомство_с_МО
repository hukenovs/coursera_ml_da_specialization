[ЗАСТАВКА] Здравствуйте. Меня зовут Евгений Соколов, и я рад приветствовать вас на курсе «Обучение на размеченных данных». Это второй курс специализации «Машинное обучение и анализ данных», в котором мы начнем знакомиться, собственно, с машинным обучением. Центральной темой этого курса является обучение с учителем. На самом деле, мы немного затрагивали эту тему, когда говорили про интерполяцию в прошлом курсе. Интерполяция — это значит восстановление функции по нескольким точкам, в которых известны ее значения. Обучение с учителем — это тоже восстановление общей закономерности по конечному числу примеров. Хотя постановки задач похожи, у них есть много отличий в том, как они решаются и какие требования выдвигаются к решению. Мы будем обсуждать эти различия в нашем курсе. Давайте разберем не очень сложный пример, на котором поймем, в чем заключается обучение с учителем и машинное обучение. Представьте, что у нас есть некоторый сайт про фильмы, на который можно зайти, найти страницу нужного фильма, почитать про него, когда он снят, кто в нем играет, какой бюджет был у этого фильма и, возможно, даже купить его и посмотреть. И есть некоторый пользователь, который заходит на наш сайт, находит страницу нужного ему фильма, читает и задается вопросом, смотреть или не смотреть, интересен ему этот фильм или не интересен. И мы хотим понять это за него. Как это можно сделать? Есть несколько подходов к решению. Подход первый, самый глупый — это дать пользователю посмотреть этот фильм. Понятно, что он потратит 1,5–2 часа, фильм может не понравится, он будет недоволен. Подход второй — показывать ему случайную рекомендацию. То есть говорить понравится или не понравится просто из генератора случайных чисел. Подход тоже не самый лучший: пользователь может смотреть фильм, он опять ему не понравится, и он будет недоволен нашим сайтом. Наконец, можно пригласить психолога-киномана разрешить ситуацию. Этот человек оценит пользователя, поймет, что ему нравится, а что — нет, оценит фильм, вспомнит про него всё и поймет, кому он может нравиться, сопоставит эту информацию и выдаст пользователю рекомендацию, смотреть или не смотреть. Этот подход довольно сложный. Скорее всего, таких специалистов не очень много в мире, и будет сложно отмасштабировать это на миллионы пользователей нашего сайта. Но на самом деле это и не нужно. Давайте поймем, что у нас много примеров, много ситуаций, когда другие пользователи заходили на страницу фильмов, принимали решение посмотреть фильм и дальше ставили оценку, по которой можно понять, понравилось им или не понравилось. Это примеры, та информация, из которой можно восстановить общую зависимость. В этом и заключается задача машинного обучения. Давайте введем пару обозначений, которыми мы будем пользоваться в нашем курсе. Машинное обучение — это раздел математики, поэтому в нем, конечно же, есть место формулам. Объектом будем называть то, для чего нужно сделать предсказание. В нашем примере объектом является пара, состоящая из пользователя и фильма, и для нее нужно предсказать, понравится ли этот фильм этому пользователю. Объекты будем обозначать маленькой буквой x. Далее, пространство объектов — это множество всех возможных объектов, для которых может понадобиться делать предсказание. В нашем случае это множество всех возможных пар «пользователь-фильм». Пространство объектов будем обозначать буквой x красивое. Ответом будем называть то, что нужно предсказать. В нашем случае ответ — это понравится пользователю фильм или не понравится. Обозначать ответы будем маленькой буквой y. Наконец, пространство ответов — это множество всех возможных ответов, с которыми мы можем работать. В нашем примере это множество состоит из двух элементов: −1 и +1. −1 означает, что пользователю фильм не понравился, +1 означает, что пользователю фильм понравился. Обозначать пространство ответов будем буквой y красивое. Как мы уже выясняли с вами в прошлом курсе, объекты — это сущности из реального мира, а компьютер не понимает, что это такое, он не знает, что такое пользователь или фильм, ему нужно объяснить эти объекты с помощью чисел, которые компьютер уже может понимать. Признак — это некая числовая характеристика объекта, а совокупность всех признаков, которых d штук, называется признаковым описанием объекта. Кстати, хотя я говорю, что признак — это число, мы с вами увидим, что есть и другие случаи, когда признак — это элемент множества, или строка, или что-то еще, но всё это — нечто, понятное компьютеру. Признаковое описание — это d-мерный вектор и можно с ним работать как с вектором, складывать, умножать на числа и так далее, то есть это — некий объект линейной алгебры. В нашем примере признаки могут быть самые разные: прошлые оценки этого пользователя другим фильмам; его анкетные данные; оценки, которые другие пользователи ставили этому фильму и так далее. Центральным понятием машинного обучения является обучающая выборка. Это то, это те примеры, на основе которых мы будем строить общую закономерность. Обучающая выборка обозначается большой буквой X и состоит из l пар объектов и ответов. xi-тое — это i-тый объект обучающей выборки, yi-тое — это истинный ответ на нем, то, что нужно предсказать. Иногда отдельный большой вопрос — это как собрать обучающую выборку, откуда ее взять. В нашем случае это довольно просто: она будет состоять из прошлых событий, когда пользователь оценивал какой-то фильм. Наконец, нам нужно что-то, что будет делать предсказания, что-то, что поможет нам решать нашу задачу с фильмами и пользователями. Это называется алгоритмом или моделью и обозначается a(x). Это, по сути, функция, которая переводит объекты в ответы, которая отображает пространство объектов в пространство ответов. a(x) принимает на вход, собственно, объект x. Кстати, слово «алгоритм» обозначает, что эта функция должна быть легко реализуема на компьютере, что ее должно быть легко использовать в системах машинного обучения. Простым примером алгоритмов являются линейные алгоритмы, о которых мы тоже говорили в прошлом курсе. Идея линейных алгоритмов очень простая: давайте возьмем все признаки и сложим их с некоторыми весами, и еще прибавим некоторую, некоторый константный коэффициент w0. Поскольку такая линейная комбинация признаков — это, по сути, любое вещественное число, а в нашей задаче ответов всего два, −1 и +1, понравился фильм или не понравился, то нужно взять знак от этой суммы, то есть для задачи классификации, для задачи определения понравится фильм или не понравился, алгоритм будет иметь вид знака от линейной комбинации всех признаков объекта. Давайте поймем, что не все алгоритмы одинаково подходят для решения нашей задачи. Например, рассмотрим константный алгоритм a(x) = 1. Алгоритм, который для всех пар «пользователь-фильм» говорит, что фильм этому пользователю понравится. Понятно, что это довольно бесполезный алгоритм, который вряд ли принесет пользу нашему сайту, поэтому нам нужно ввести некоторую характеристику полезности, характеристику качества алгоритма для данной конкретной задачи. Эта характеристика называется функционалом ошибки и обозначается как Q. Q принимает на вход алгоритм и выборку и возвращает некоторую характеристику того, насколько хорошо работает данный алгоритм на данной выборке. В нашем случае это может быть, например, доля неправильных ответов, то есть берем всю выборку x и смотрим, на какой доле пар «пользователь- фильм» наш алгоритм ошибся, выдал неправильное предсказание. Понятно, что чем меньше будет такая доля неверных ответов, тем лучше. Обратите внимание на еще один факт: функция Q называется функционалом ошибки, а не функцией. Функционал, потому что она принимает на вход другую функцию, алгоритм является функцией, как вы помните. Итак, задача обучения состоит в подборе такого алгоритма a, на котором достигается минимум функционала ошибки. Сразу возникает вопрос: а из какого множества нужно выбирать лучший алгоритм? Для этого вводится понятие семейства алгоритмов, которое обозначается буквой A красивое. По сути, это множество всех алгоритмов, среди которых мы будем искать лучший, тот, который лучше всего подходит для решения нашей задачи, которая составляет минимум к функционалу ошибки. Простейшим примером семейства алгоритмов являются решающие пни. Каждый решающий пень делает очень простую вещь: он берет некоторый один фиксированный признак xj-тое и сравнивает его значение на данном объекте с некоторым порогом t. Если значение признака меньше этого порога, то алгоритм возвращает ответ −1, говорит, что этому пользователю фильм не понравится. Если же значение j-того признака больше или равно порога t, то алгоритм дает ответ +1, говорит, что пользователю фильм понравится. Это очень простые алгоритмы, они могут проверять только очень простые факты вроде «Пользователь посмотреть больше трех комедий». Понятно, что здесь нужно проверять более сложные, парные взаимодействия, например, «Пользователь посмотрел больше трех комедий» и «Данный фильм является комедией». Но решающие пни на это не способны, для этого нужны более сложные семейства алгоритмов. Тем не менее, решающие пни пригодятся нам в этом курсе для составления сложных композиций алгоритмов. Обратите внимание на одно обозначение, которое используется на этом слайде. Это квадратные скобки, скобки Айверсона или нотация Айверсона. Внутри скобок Айверсона находится некое логическое выражение, например, «значение j-того признака меньше порога t». Если это выражение — верное, то значение скобок равно 1, если же значение неверное, то значение скобок равно 0. Итак, чтобы заниматься машинным обучением, нужно уметь отвечать на три вопроса: как измерять качество, какой функционал ошибки использовать в данной задаче; какое взять семейство алгоритмов, из чего выбирать оптимальный алгоритм для данной задачи; и, наконец, как этот выбор производить, как делать обучение алгоритма? При этом есть ряд других вопросов, которые не менее важны, но относятся скорее к анализу данных, а не к машинному обучению. Анализ данных — это более широкая область науки, которая включает в себя множество различных эвристик, очень полезных для этой работы. Например, как сформировать признаки, какие выбрать признаки, чтобы на них задача решалась лучше всего, или как готовить признаки, как их предобрабатывать, как они должны выглядеть, чтобы алгоритм хорошо обучался на них, или, например, какую выбрать метрику, чтобы алгоритм не только хорошо настраивался, но и приносил реальную экономическую пользу заказчику. В этом уроке мы поговорим о совсем базовых понятиях: о постановках задач машинного обучения и о том, какие признаки бывают машинного обучения. Это очень простые и базовые вещи, которые пока не имеют отношения к реальным задачам, но нам очень важно о них поговорить, чтобы использовать общую терминологию, говорить на одном языке, а уже в следующем уроке мы перейдем к реальным примерам и к реальным семействам алгоритмов, а именно к линейным.

[ЗАСТАВКА] В этом видео мы поговорим о том, какие бывают типы задач обучения на размеченных данных или обучения с учителем, и обсудим несколько их примеров. В прошлый раз мы обсуждали общую постановку задачи обучения с учителем. В ней есть обучающая выборка, то есть набор пар «объект и ответ» — объектов и ответов, которые нужно предсказывать для этих объектов. И нужно найти такой алгоритм из семейства алгоритмов A (красивое), на котором будет достигаться минимум функционала ошибки, то есть найти такой алгоритм, который будет лучше всего решать нашу задачу, лучше всего подходить к нашей обучающей выборке. В зависимости от того, какие именно ответы должны возвращать алгоритмы в этой задаче, зависит, с каким типом задачи мы имеем дело. Иными словами, тип задачи определяется пространством ответов, которое мы обозначали Y (красивое). Замечу, что бывают и другие задачи, не только обучения с учителем, но об этом в следующем видео. А первый пример, о котором мы поговорим, это задача бинарной классификации. В этих задачах пространство ответов состоит из всего двух элементов, их обычно обозначают как 0 и 1 или –1 или +1. Множества объектов, которые относятся... которые имеют один ответ, например ответ «–1», называются классом, и говорят, что нужно уметь относить объект к одному из двух классов или классифицировать эти объекты. Давайте рассмотрим простой пример. Если у нас каждый объект описывается всего двумя признаками, то есть выборка двумерная, то можно эту выборку нарисовать. По одной оси отложим значение первого признака, по другой — значение второго признака, и каждая точка в этих осях будет обозначать один объект обучающей выборки. По сути, задача классификации состоит в том, чтобы провести некоторую разделяющую кривую, которая будет отсекать один класс от другого, разделять синие и красные точки. Примеров задачи бинарной классификации очень много. Например, можно предсказывать, понравится ли пользователю фильм — то, о чем мы уже говорили. Или, например, вернет ли клиент кредит или не вернет — задача кредитного скоринга, очень популярная в банковской сфере. Или, например, нужно ли делать пациенту операцию, будет ли операция иметь долгосрочный положительный эффект. Или можно просто предсказывать, качественное ли вино, сделано ли оно по всем канонам или это дешевая подделка. Классов может быть не два, а больше. Задача, в которой конечное число классов, например K штук, называется многоклассовой классификацией. Визуально это означает следующее. Допустим, признаков все еще два, но при этом цветов точек (а цвет обозначает класс точки, класс объекта) будет больше. В этом случае надо провести не одну разделяющую кривую, а много. Для каждого класса будет своя кривая, которая отсекает этот класс от всех остальных. Понятно, что это уже более сложная задача. Какие есть примеры задач многоклассовой классификации? Например, можно пытаться понять, из какого сорта винограда сделано вино. Понятно, что сортов конечное количество, значит это многоклассовая классификация. Или, например, можно определять тематику научной статьи. Из какой области эта статья? Она про математику, про физику, про биологию или, может быть, про философию? Или, например, можно пытаться понять по фотографии, какой тип машины там присутствует: мотоцикл, легковая или грузовая машина? Это может понадобиться, чтобы автоматически определять, какую плату за проезд по платной дороге взять с автомобилиста по фотографии его машины возле КПП. Классов может быть не конечное число. Если классов бесконечное количество, например ответом может быть любое вещественное число, то мы имеем дело с задачей регрессии. Собственно, в задачах регрессии пространство ответов — это все вещественные числа. Давайте разберем простой пример. Нам нужно предсказать рост человека по его весу. В этом случае по оси x мы отложим вес человека в килограммах — признак, по оси y отложим ответ — рост человека в сантиметрах. Каждая точка будет соответствовать одной паре «объект–ответ». В нашем примере очень легко видеть, что зависимость почти линейная. Можно провести прямую, которая будет очень хорошо предсказывать рост человека по его весу. Есть и более сложные примеры задач регрессии. Например, предсказание температуры на завтрашний день. Понятно, что температура — это вещественное число. Или, например, предсказание прибыли магазина в следующем году, или определение возраста человека по его фотографии. Еще одним примером задачи обучения с учителем является задача ранжирования. Это довольно тяжелая задача, о которой мы не будем говорить в этом курсе, но знать о ней очень полезно. Это задача, с результатом решения которой вы сталкиваетесь каждый день, когда ищете что-то в поисковике, например в Яндексе. Ранжирование поисковой выдачи заключается в следующем. Пользователь вводит некоторый запрос. Например, ему хочется найти картинки с котятами. И у нас есть множество всех страниц в Интернете, которые нам известны. Это миллиарды или даже триллионы страниц. И нужно отсортировать все эти страницы по тому, насколько они подходят под запрос пользователя, насколько они отвечают на его вопрос. Понятно, что очень непросто отсортировать, отранжировать такое количество документов, но эта задача вполне решаемая. Итак, мы обсудили основные постановки задач обучения с учителем. Это бинарная многоклассовая классификация, это регрессия. Также мы немножко поговорили о ранжировании. А в следующем видео поговорим о задачах обучения без учителя.

[ЗАСТАВКА] В этом видео мы поговорим о том, какие бывают постановки задач в машинном обучении помимо обучения с учителем. И рассмотрим несколько примеров задач обучения без учителя. Итак. Обучением с учителем называются такие задачи, в которых у нас есть и объекты, и истинные ответы на них. И нужно по этим парам восстановить общую зависимость, построить алгоритм или модель, которые будут предсказывать ответы по объектам. Задача обучения без учителя — это такая задача, в которой есть только объекты, ответов нет, и при этом с этими объектами нужно что‐то сделать. Также есть и промежуточные постановки. Например, частичное обучение. В этом случае у нас есть объекты, но ответы известны лишь на части объектов. И нужно как‐то, имея эту информацию, тоже восстановить общую зависимость, построить модель. Или, например, активное обучение. Это задача, в которой есть объекты, но получать ответ для объекта, истинный ответ, очень дорого, очень тяжело. Поэтому алгоритм должен уметь определять, на каких объектах ему надо знать ответ, чтобы лучше всего обучиться, построить наилучшую модель. В этом видео мы обсудим 3 примера постановки задачи обучения без учителя, чтоб вы понимали важность этого класса задач. Первым примером будет задача кластеризации. В этом случае у нас есть некий набор объектов, и нужно сгруппировать их, найти группы похожих объектов. У этой задачи есть 2 проблемы. Проблема первая: мы даже зачастую не знаем количество этих групп, мы не знаем, сколько кластеров имеется в наших данных. А во‐вторых, мы не знаем правильных ответов, мы не знаем истинные кластеры, которые нужно выделять. Поэтому задача решается очень тяжело, здесь нельзя измерить точно качество решения. Кстати, вот этим она и отличается от задачи классификации. В классификации тоже нужно относить объект к одной из групп, но там есть примеры объектов этих групп. Поэтому задача классификации гораздо проще, в ней можно померить качество решения. Примеров задачи кластеризации очень много. Например, эта сегментация пользователей, например интернет‐магазина или мобильного оператора. Им зачастую интересно найти группы похожих пользователей, чтобы дальше, например, заниматься маркетингом для каждой группы в отдельности. Понять, что такого особенного в этой группе, что все пользователи в ней схожие, и ориентировать рекламу именно на этот сегмент, на эту группу. Или, например, можно искать группы похожих пользователей социальных сетей. Но при этом кластеризовать — группировать — можно не только людей. Например, можно кластеризовать гены, пытаясь найти такие группы генов, которые одновременно включаются или выключаются у разных людей в разных условиях. Второй пример задачи обучения без учителя — это задача визуализации. Здесь нам нужно нарисовать многомерную выборку — выборку, которая описывается большим числом признаков. То есть надо уметь многомерную точку отразить в двумерное пространство, то есть на плоскость, или в трёхмерное пространство, то есть в пространство. При этом отобразить нужно так, чтобы визуализация, изображение нашей выборки в двумерном или трёхмерном пространстве отражало структуру исходной многомерной выборки. Чтобы глядя на это изображение, можно было понять, как устроены эти данные. что с ними можно делать. Также обычно есть требование, чтобы эта визуализация была красивой, чтоб на неё было приятно смотреть. Классическим примером задачи визуализации является визуализация data set'а MNIST. Это data set, в котором были отсканированы рукописные начертания всех цифр — от 0 до 9. Понятно, что каждый скан, каждое изображение, характеризуется сотнями пикселей. Но при этом, если грамотно отразить эту многомерную выборку на плоскость, то цифры вполне будут группироваться. Например, цифра 0 будет отдельным облаком где‐то. Причём особенностью хорошей визуализации будет то, что даже начертания одной и той же цифры будут разделяться на разные группы в зависимости от того, как именно написана эта цифра, например с засечкой или без. Третий пример задачи обучения без учителя — это задача обнаружения аномалий, поиска аномалий. В ней требуется обнаруживать, определять, что данный объект не похож на все остальные, что он является аномальным. При этом при обучении у нас есть только примеры обычных, неаномальных объектов, а примеров аномальных либо нет вообще, либо настолько мало, что невозможно воспользоваться классическими методами обучения с учителем. При этом задача очень важная. Например, можно пытаться обнаружить что в самолёте есть поломка по показателям сотен датчиков, расположенных в нём. Такое обнаружение позволит избежать аварии, понятно, что это очень полезно. Или, например, если у нас есть интернет‐сайт, например интернет‐магазин или поисковый сайт, можно пытаться, опять же, по многим показателям понять, что произошла поломка, аномалия, что с сайтом нужно что‐то делать, нужно его срочно чинить. Или, например, если есть некоторая модель машинного обучения, которая делает прогнозы, скажем, понравится ли пользователю фильм или нет, можно пытаться следить за ней, понимать, хорошо ли он делает предсказания, или что‐то поломалось. Например, из‐за того, что распределение одного из признаков поменялось. Итак, мы обсудили 3 примера постановки задач обучения без учителя: кластеризацию, визуализацию и поиск аномалий. В этом курсе мы не будем о них говорить, им будет посвящен следующий курс — «Поиск структуры в данных», приходите. А в следующем видео мы поговорим о том, какие бывают признаки в задачах машинного обучения.

[ЗАСТАВКА] В этом видео мы поговорим о признаках машинного обучения. Существует несколько классов, несколько типов признаков, и у всех свои особенности. Все нужно по-разному готовить и по-разному учитывать в алгоритмах машинного обучения. В этом видео мы больше поговорим о терминологии, чтобы закрепить ее, а о самих особенностях работы с этими признаками будем говорить в следующих уроках. Также мы немного затронем вопросы, какие проблемы могут встретиться в тех или иных видах признаков. Итак, как вы уже знаете, признак — это некоторое число или другая понятная компьютеру сущность, которая как-то описывает объект в доступной форме. Множество значений j-го признака будем обозначать буквой Dj. И первый тип признаков, о которых мы поговорим, это бинарные признаки. Это самый простой тип признаков, которые принимают значение 0 или 1. Всего два значения. Например, в задаче кредитного скоринга мы можем смотреть, клиент получает зарплату выше, чем в среднем по городу, или нет? Если ответ на вопрос «да, его зарплата выше, чем средняя», то значение признака равно 1, если ответ «нет», то значение признака равно 0. У него два значения, он является бинарным. Или другой пример: если мы классифицируем изображение фруктов, пытаясь понять, какой именно фрукт там нарисован, то мы можем сделать признак, который отвечает на вопрос: фрукт зеленый или нет? 1 — если зеленый, 0 — если нет. Тоже бинарный признак. Чуть более сложный класс признаков — это вещественные. В этом случае множество значений — это все вещественные числа. Например, в задаче кредитного скоринга это может быть возраст клиента — понятно, что это вещественное число. Или в задаче оценивание стоимости квартиры можно рассматривать признак «площадь квартиры». Или же в задаче предсказания оттока клиентов мобильного оператора можно смотреть на признак «количество звонков в колл-центр за последний месяц». Следующий класс признаков — категориальные. В случае с категориальными признаками множество значений — это некоторое неупорядоченное множество. Это означает, что мы можем сравнивать элементы этого множества лишь на равенство, на совпадение, но при этом нельзя сравнивать их между собой на больше или меньше. Простой пример категориального признака — это цвет глаз человека. Понятно, что есть несколько вариантов цвета глаз, и их нельзя сравнивать. Нельзя сказать, что зеленый цвет больше или меньше, чем голубой. Разумеется, можно вдариться в подробности и, например, сравнивать цвета по длине волны, но не факт, что во всех задачах такое сравнение имеет смысл. Еще один пример категориального признака — это город, где родился клиент банка, который сейчас просит дать ему кредит. Или еще один пример — это образование. Понятно, что образование может пойти по нескольких веткам, например высшее образование и среднее профессиональное, и их тоже нельзя сравнивать между собой. При этом опять же в некоторых задачах можно ввести осмысленный порядок на этих значениях, но об этом чуть позже. Категориальные признаки очень трудны в обращении. До сих пор появляются способы учета этих признаков в тех или иных методах машинного обучения. Частным случаем категориальных признаков являются порядковые признаки. В этом случае множество значений признака — это некоторое множество, которое является упорядоченным. То есть можно сравнивать значения между собой, но нельзя измерять расстояние между ними. Этим порядковые признаки отличаются от вещественных. Пример порядкового признака — это роль в фильме. Актер может иметь роль первого плана, роль второго плана, роль в массовке, и эти виды ролей легко сравниваются между собой. Или, например, тип населенного пункта — это тоже порядковый признак. Есть вполне определенный порядок на всех типах населенных пунктов: деревня, город, областной центр, столица — что-то в этом духе. Как я уже говорил, на образовании можно ввести порядок в некоторых задачах. Например, банк в задаче кредитного скоринга может ввести порядок на типах образования в зависимости от того, клиенты с каким образованием лучше или хуже возвращают кредиты. И, например, может получиться, что два высших образования хуже, чем одно высшее образование. Наконец, последний тип признаков на сегодня — это множествозначные признаки. Множествозначный признак — это такой признак, значение которого на объекте — это подмножество некоторого множества. Например, можно рассматривать множество всех фильмов, которые когда-либо вышли (это, наверное, сотни тысяч фильмов), и значение признака для одного посетителя сайта — это множество фильмов, которые он посмотрел, это подмножество множества всех фильмов. Или, например, в задачах анализа текстов если мы рассматриваем текст, например, сообщения в Twitter, один объект — это некоторое сообщение, некоторый набор слов, который является подмножеством Большого словаря. Давайте теперь поговорим о двух проблемах, с которыми можно столкнуться при работе с признаками. Первая из них — это выброс. Выбросом называется такой объект, значение признака на котором отличается от значений признака на большинстве объектов. Например, на этом графике нарисованы распределения некоторого признака, и видно, что у большинства объектов значение этого признака концентрируется вокруг 5, но при этом на одном или двух объектах оно равно 15. Эти объекты — выбросы. Значения на них не вписываются в общее распределение. При этом если вы будете настраивать алгоритм машинного обучения на такой признак, у него, скорее всего, будут проблемы. Он будет пытаться хорошо работать и с распространенными значениями признака, и с выбросами. Но при этом поскольку выбросы, скорее всего, приходят из другого распределения, они описываются другими закономерностями, у алгоритма могут возникнуть большие проблемы при этом. Иногда выбросы лучше просто выкинуть. Дело может быть даже не в выбросах, а в том, как распределен признак. Например, в задаче кредитного скоринга можно посмотреть на признак «город, где родился клиент». При этом из каких-то городов у банка будет много клиентов, например из Москвы или Питера, а из каких-то небольших городов с населением в несколько десятков тысяч может быть мало клиентов — один или два. В этом случае статистики по этим городам будет слишком мало, чтобы накопить какие-то данные, чтобы делать выводы на основе того, что клиент пришел из этого города. С этим тоже могут быть проблемы, и надо как-то их решать. Мы будем говорить об этом, когда будем обсуждать работу с категориальными признаками. Если же признак вещественный, то проблема может быть в его распределении следующая: представьте, что мы смотрим на распределение стоимости книг в интернет-магазине, который торгует книгами. Оно будет выглядет примерно, как на этой гистограмме. У большинства книг цена будет не очень большая — несколько сотен рублей, и здесь почти все распределение концентрируется. Но при этом есть ряд книг, которые довольно дорогие — стоят тысячу или несколько десятков тысяч рублей. Они составляют «хвост» распределения, и он является довольно тяжелым. При работе с таким распределением, если алгоритм будет настраиваться на него, у него тоже могут возникнуть проблемы. Здесь нужно либо работать по отдельности с большинством книг, у которых цена вписывается в несколько сотен, и с дорогими книгами, или же как-то преобразовывать распределение этого признака, чтобы оно было более нормальным. Итак, мы обсудили основные типы признаков: это бинарные; вещественные; категориальные и порядковые; и множествозначные. Поговорили и о том, какие примеры этих признаков могут быть. А также обсудили две проблемы с признаками: они могут иметь выбросы, значение которых надо лучше выкинуть, а также могут быть проблемы в самом распределении признака, например слишком редкие значения или перекошенное распределение. На этом вводные лекции заканчиваются, и вам предстоит сделать задание по программированию, где вы посмотрите на реальные данные и заметите, какие могут быть проблемы в этих данных. А в следующем уроке мы уже начнем говорить о о реальных алгоритмах машинного обучения, а именно о линейных моделях.