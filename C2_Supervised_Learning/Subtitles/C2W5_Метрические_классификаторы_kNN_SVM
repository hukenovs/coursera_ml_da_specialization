В этом уроке мы собрали методы, которые не освещались до сих пор, но знать о них, наверное, стоит. В первую очередь это метрические алгоритмы, то есть алгоритмы, которые используют расстояние между объектами. В них нет ничего особенного, они безумно просты, но это также и плюс. Кроме того, мы узнаем, насколько гибки эти методы и как легко они адаптируются для разных задач. Еще один метод, который оказался в этом уроке — метод опорных векторов, или SVM. На самом деле, вы с ним уже знакомы, ведь это просто частный случай линейного классификатора. Но исторически этот метод был придуман немного иначе. Так как у нас курс про машинное обучение, а не про историю машинного обучения, мы посвятим SVM'у всего два видео, в которых расскажем основные моменты для расширения кругозора.

[ЗАСТАВКА] В этом видео мы начинаем знакомство с метрическими алгоритмами. Метрические алгоритмы — это методы, которые предполагают, что в пространстве признаков введено понятие расстояний, или, как говорят в математике, метрики. Мы начнем с самого простого метода — метода ближайшего соседа для задачи классификации. Он устроен действительно очень просто. Давайте попробуем понять, как мы можем отнести какую-то новую точку к какому-то классу. Давайте посмотрим, как она расположена относительно уже известных из обучающей выборки точек. Посмотрим, какая точка из обучающей выборки ближе, и отнесем новую точку к тому же самому классу. Вот и весь метод. Можно этот метод модифицировать. Действительно, принимать решения по одной точке может быть не очень надежно. Но давайте посмотрим на k ближайших точек. И посмотрим, какой класс среди них доминирует, ну то есть кого среди них больше, и отнесем новую точку к этому классу. В этот алгоритм можно логично добавить веса объектов. Веса могут зависеть от номера соседа, ну то есть первый сосед, второй сосед, в зависимости от близости, а могут зависеть от расстояния до соседа. И когда у нас введена функция весов, мы определяем класс очень просто. Просто берем и подсчитываем сумму весов для одного класса, и сумму весов для другого класса. И принимаем решение, к какому классу отнести на основе того, какая сумма получилась больше. Можно придумать и еще более простой метрический классификатор, например, посчитав центр одного класса, центр другого класса, ну просто как среднее арифметическое точек, которые входят в один и в другой класс, и посмотрев, какой центр ближе к новой точке. К этому классу и будем относить. С помощью метода k ближайших соседей можно решать так же задачу регрессии. Здесь всё то же самое, только теперь мы суммируем не просто веса объектов, а суммируем веса, умноженные на значение функции, которую мы хотим приблизить, в этих объектах. Ну и, конечно, надо нормировать всё на сумму всех весов. Обратите внимание — в зависимости от того, какие веса мы выбираем, наше решение может обладать какими-то дополнительными свойствами. Ну, например, если мы будем решать задачу регрессии и в качестве веса будем использовать 1 делить на расстояние до объекта, то тогда наш результат будет заметно переобучен. Само собой, в каждой точке из обучающей выборки вес этой точки будет бесконечно большим. И он будет забивать веса других точек. Поэтому к выбору функции весов нужно подходить очень внимательно. Подведем итог. Мы познакомились с методом k ближайших соседей, выяснили, что в него очень органично можно добавлять веса объектов и узнали, что его можно использовать как для задач классификации, так и для задач регрессии.

[ЗАСТАВКА] Итак, мы познакомились с методом k ближайших соседей и теперь можно подумать о том, как же выжать максимум из его качества. Ну конечно, можно попробовать понастраивать какие-нибудь параметры. Но в то же время kNN обучается очень просто — он просто запоминает обучающую выборку. Однако есть параметры, которые выбирает исследователь. Это количество соседей, веса объектов и метрика. Эти параметры можно задать таким образом, чтобы качество работы алгоритма было как можно лучше. Конечно, не стоит их подбирать на обучающей выборке, но можно это сделать на специально отложенной тестовой, или еще лучше по качеству на кросс-валидации. Ну давайте посмотрим на количество соседей k. Как вы видите, самое подходящее k по качественно обучающей выборке, это k=1. Но это и понятно. Если мы пытаемся классифицировать объект из обучающей выборки, то мы просто находим ближайший к нему объект (а это он сам) и смотрим класс, к которому он принадлежит. Именно поэтому и не стоит подбирать такие парамеры по обучающей выборке. При этом видно, что качество и на обучающей выборке, и на тестовой не выглядит как совсем прям монотонная зависимость, а есть небольшие колебания, но в то же время обычно в общем поведение следующее: сначала качество на контроле становится все лучше с ростом k, а затем качество начинает ухудшаться. И вот нужно выбрать то самое k, при котором качество самое вменяемое. Теперь поговорим о весах объектов. Ну во-первых, понятно, что функция весов не должна возрастать с ростом номера объекта. Ну и понятно, что мы можем не заморачиваться и просто взять константные веса для всех объектов, получив самый стандартный kNN. Важно об этом помнить и не пытаться выдумывать какие-то сложные веса до того, как попробуете простой метод. Но если мы все же понимаем, что хотим попробовать разные веса, то подойдут веса вида число от 0 до 1 в степени i, где i — это номер соседа, или 1/i, или 1/i + какая-то константа, или можно вообще знаменатель возвести в какую-то степень. И вот эти константы, их тоже можно поварьировать и подобрать такие, при которых качество будет самое подходящее. Бывают и не очень удачные варианты функции весов. Ну например, если мы возьмем линейно зависящую от номера соседа функцию, то, например, при k=4, то есть для случая четырех соседей, если первый и последний сосед из одного класса, а второй и третий сосед из другого класса, алгоритм просто не сможет принять решение, потому что веса классов получатся одинаковые. Это, конечно, не значит, что эта функция вовсе не применима на практике, но важно учитывать, что ваша функция весов может иметь такого рода особенности. Ну а теперь посмотрим на веса объектов как на функцию от расстояния. Опять же мы помним, что можно было взять просто константы, но и можно придумать что-нибудь достаточно простое, например, 1 делить на расстояние. И вот давайте посмотрим, как будет выглядеть решение задачи регрессии в одном и в другом случае. Ну для простоты от одной переменной, то есть от одного признака. Это пример из прошлого видео, и мы уже обращали внимание на то, что вариант с весами получается очень переобученный. Действительно, когда мы пытаемся спрогнозировать зависимость в той точке, которая у нас уже есть в обучающей выборке, вес этого примера получается бесконечно большим, этот пример просто забивает другие и определяет результат. Поэтому нужно достаточно аккуратно выбирать веса. Можно, например, пробовать сгладить эту функцию весов, добавив в знаменателе какую-нибудь константу. Можно попробовать опять же возвести знаменатель в какую-то степень или попробовать функцию вида q в степени d, где q — это число от 0 до 1, а d — это расстояние. Есть и более общий подход к придумыванию функции весов, зависящих от расстояний, он использует функции, называемые ядрами, но на нем мы сейчас не будем останавливаться. Подведем итог: даже при том, что всё обучение kNN сводится к запоминанию обучающей выборке, есть параметры, которые можно понастраивать. Настраивать их нужно не на обучающей выборке, а на отложенной выборке, или по качеству на кросс-валидации. Мы обсудили выбор количества соседей и весов объектов. В следующем видео мы поговорим про расстояние, или иначе говоря, метрики, которые используется на практике.

[ЗАСТАВКА] Поговорим про метрики. Что такое расстояние или метрика? Ответить на этот вопрос можно аксиомами метрики, с которыми все знакомятся в университетах. Но это совершенно неважно для нас сейчас. Для нас главное, чтобы работало. Главное, чтобы объекты, которые меньше похожи друг на друга, были дальше друг от друга, то есть расстояние между ними получалось больше, а выполнение аксиом совершенно некритично. Но самая простая метрика, которая приходит на ум, это евклидова метрика. Это просто корень из суммы квадратов разности координат. Можно попытаться ее обобщить. Ну, например, точно также мы могли бы рассматривать сумму модулей отклонений координат. А в общем случае могли рассматривать сумму q-тых степеней модуля отклонений, возведенную в степень 1 делить на q. Как посмотреть на метрику? Ну, можно просто построить шар или круг (если на плоскости, полученной в этой метрике), ну то есть множество точек, удаленных, например, от начала координат не более чем на фиксированное расстояние, например на единицу. И окажется, что в случае манхэттенского расстояния, ну то есть сумма модулей разности координат, шар или круг будет выглядеть ромбовидно. В случае степени 2 мы получаем евклидову метрику и получаем обычный круг и обычный шар. Если же степень метрики Минковского устремить к бесконечности, то метрика выродится в максимум из модулей разности координат, и, соответственно, круг будет просто квадратом. Также часто используется так называемая косинусная мера. Это просто косинус угла между векторами, который совсем несложно посчитать, используя скалярное произведение и длины векторов. И обратите внимание, это функция близости, а не расстояния. То есть, чем больше похожи объекты друг на друга, тем больше будет значение этой функции. Также используется коэффициент корреляции, который отличается от косинусной меры лишь тем, что из каждого вектора вычитают среднее значение его координат. Коэффициент корреляции можно использовать, как функцию близости. Косинусная мера часто используется для анализа текстов, а коэффициент корреляции часто используется в рекомендательных системах. Но об этом мы еще поговорим позднее. Конечно, существует и много других функций близости, которые в разной степени учитывают разные различия между векторами. И подводя итог, можно сказать, что метрики бывают разные даже в обычном строгом математическом смысле. А так как нам интересно не то, что хорошо для математиков, а то, что работает на практике, то у нас еще больше возможных вариантов. Иногда в задаче может быть понятно, какая метрика лучше для нее подходит. Но, как правило, нужно попробовать перебрать разные. Некоторые варианты часто используются в каких-то прикладных областях, но это не отменяет того, что всегда полезно поэкспериментировать.

[ЗАСТАВКА] В этом видео мы обсудим одно явление, очень важное для метрических алгоритмов. Какую роль играет расстояние в метрических методах? Оно должно показывать, насколько объекты похожи друг на друга или насколько не похожи? Таким образом, мы ожидаем, что расстояние тем больше, чем меньше чего-то общего между объектами. Но в то же время в машинном обучении признаков часто очень много. И возникает вопрос: нет ли каких-то особых свойств у расстояния в многомерном пространстве, в пространстве высокой размерности? Но вот давайте рассмотрим одно соображение на этот счет. Допустим, у нас есть некоторый вектор x1, и вектор x2 от него отличается в каждой координате, но совсем чуть-чуть. А вектор x3 отличается от x1 только в одной координате, но существенно. Вот какой из векторов будет ближе к x1? Понятно, что ответ зависит от метрики. Но в то же время понятно, что когда признаков очень много, маленькие различия могут накопиться и стать более значимыми, чем большое различие в одном признаке. А может быть вы этого не хотели. Другое соображение: на плоскости легко представить себе комбинацию из трех точек, равноудаленных друг от друга, то есть не группирующихся в какой-то одной области, которую мы, например, хотим интерпретировать как область, в которой представлены точки какого-то одного класса. В трехмерном пространстве тоже легко придумать конструкцию из четырех точек, но это будет просто правильный тетраэдр. В N-мерном пространстве можно придумать конструкцию из N + 1 точки. И все это нас наводит на мысли, что тогда, когда количество признаков сравнимо с количеством объектов, объекты в принципе могут оказаться равноудаленными друг от друга или практически равноудаленными. И еще одно соображение. Давайте представим себе вектор размерности N из бинарных признаков (ну, для простоты мы будем рассматривать бинарные признаки). Тогда всевозможных комбинаций признаков будет 2 в степени n. Это значит, что с ростом N экспоненциально увеличивается и количество объектов, которые должны быть в обучающей выборке, чтобы покрыть все возможные ситуации на ту же самую долю. Это означает, что количество необходимых данных растет экспоненциально. Ну и на эту тему есть очень хорошая, красивая иллюстрация. Давайте рассмотрим куб с ребром 1 и в нем рассмотрим куб с ребром 1/2. Ну и посмотрим, какую часть от объема большого куба будет составлять маленький куб. В одномерном пространстве всё вырождается в отрезке, и мы видим, что маленький куб, ну точнее маленький отрезок, будет составлять 1/2 от длины большого. В двумерном случае мы получаем квадраты, и видим, что площадь маленького квадрата будет составлять уже 1/4 от площади большого квадрата. В трехмерном случае объем будет составлять 1/8. И в N-мерном случае объем будет составлять все меньшую и меньшую долю. Можно рассмотреть чуть более показательный вариант, когда маленький куб будет иметь длину ребра 0,99. И понятно, что в случае N-мерного пространства часть, которая составляет маленький куб от большого, будет равна 0,99 в степени n. То есть при N, стремящемся к бесконечности, доля объема маленького куба стремится к нулю. Что это значит? Это значит, что в пространствах высокой размерности основная часть объема концентрируется вблизи границы области. То есть получается, что объекты оказываются очень часто примерно одинаково удалены друг от друга. Подведем итог. В многомерных пространствах расстояние между объектами ведет себя не совсем так, как мы привыкли. С ростом числа признаков необходимый объем обучающей выборки растет экспоненциально, и это становится серьезной проблемой для метрических алгоритмов. Объекты часто оказываются на примерно одинаковом расстоянии друг от друга. Вместе все эти наблюдения носят название «проклятие размерности». Проклятие размерности может существенно мешать применению метрических алгоритмов.

[БЕЗ_ЗВУКА] Рассмотрим задачу использования метода k ближайших соседей на практике. Допустим, нам нужно построить рекомендации фильмов. У нас есть исторические данные с оценками, которые ставили пользователи фильмам. И нам нужно для тех фильмов, которые пользователи еще не видели, спрогнозировать оценки, которые пользователи могли бы им поставить, для того чтобы рекомендовать те фильмы, которые, скорее всего, будут оценены наиболее высоко. У нас есть матрица пользователя фильма, в которой какие-то значения нам известны, а какие-то значения нет: их-то нам и нужно спрогнозировать. И вот, допустим, нам нужно прогнозировать одно из таких значений. Как мы можем действовать? Мы можем просто взять и посмотреть, какие пользователи похожи на пользователя, для которого мы делаем прогноз, и в какой степени. Тогда мы можем выделить тех, кто похож в большей степени, и тех, кто похож в меньшей степени, и присвоить им какие-то веса: тем, кто больше похож, веса побольше, тем, кто меньше, веса поменьше. И вот с этими весами мы можем просто взять и усреднить оценки, которые другие пользователи ставили этому фильму. Это и будет нашим прогнозом для исходного пользователя. Такой подход называется user-based, потому что все рассуждения в нем ведутся от рассмотрения похожих пользователей. И аналогично можно было бы придумать подход item-based, то есть смотреть на похожие фильмы и усреднять по ним. Дальше мы будем для определенности говорить про user-based подход, но для item-based все аналогично. Нам нужно ввести каким-то образом похожесть пользователей, то есть нам нужно для пользователя i и для пользователя j уметь отвечать на вопрос, насколько они близки друг к другу по интересам. Для начала давайте просто возьмем и посчитаем средний рейтинг, который ставит фильмам пользователь i и пользователь j. Теперь можно вычислить похожесть ну, например, по формуле, приведенной на слайде. Обратите внимание, это просто уже известная нам косинусная мера, в которой мы вычли из векторов, описывающих пользователей, средние значения рейтинга. Также часто применяется следующий трюк. Можно суммировать не по всем фильмам, а только по фильмам, которые видели и i-й, и j-й пользователь. Но еще можно сразу вычесть средние значения из векторов и использовать обычную косинусную меру. Теперь остается придумать, как же прогнозировать рейтинг фильма. Можно просто взять средний рейтинг, который ставят пользователи, и добавить к нему взвешенную сумму рейтингов, которые ставили другие пользователи, но, конечно, с поправкой на средние значения. Но на самом деле это может быть не очень хорошей идеей, если у нас очень много данных по большому количеству пользователей. Действительно, зачем нам считать все время суммы, в которых несколько миллионов слагаемых? Достаточно просто посмотреть на k ближайших соседей к пользователю, для которого мы делаем оценку, и просуммировать по ним. Вот так метод k ближайших соседей может быть адаптирован для задачи рекомендации. Таким образом, мы узнали, что с помощью kNN можно решать не только задачи классификации регрессии в самом обычном их виде, но и какие-то не совсем стандартные задачи. Нужно лишь уметь понимать, какую величину и по каким именно соседям оправданно усреднять, а также придумывать хорошую метрику.


[БЕЗ_ЗВУКА] Давайте познакомимся поближе с еще одним методом, который не вошел в основную часть курса, а именно с методом опорных векторов. Конечно, в каком-то виде вы с ним уже знакомы. Это просто линейный классификатор, использующий кусочно-линейную функцию потерь и L2-регуляризатор. Но на самом деле метод был придуман не из общего вида линейных классификаторов и не из обобщения с функциями потерь и регуляризаторами. Он был придуман из других довольно простых соображений. Давайте представим себе линейно разделимую выборку, то есть выборку, которую можно разделить на 2 класса (мы будем рассматривать случай бинарной классификации) с помощью гиперплоскости. Понятно, что эту гиперплоскость можно провести по-разному. И можно провести ее так, что она разделит выборку, и при этом могут быть небольшие изменения в коэффициентах плоскости, и она продолжит разделять выборку. Возникает вопрос: каким же образом ее провести так, чтобы это было более-менее оптимально? Естественная мысль — рассмотреть разделяющую полосу, ну то есть конструкцию, которая получается, если гиперплоскость подвинуть максимально к одному классу и максимально к другому классу. Тогда можно задуматься вот о чем: какие значения будет принимать скалярное произведение вектора весов w на x минус коэффициент сдвига в тех точках, которые относятся к одному классу и к другому классу и через которые проходят границы разделяющей полосы. Ну то есть получается, это самые крайние точки каждого из классов. Дело в том, что если мы возьмем и умножим вектор w и коэффициент w0 на какое-то число, то разделяющая поверхность совершенно не поменяется. В самом деле, мы как смотрели на знак некоторого выражения, так и будем смотреть — просто выражение будет умножено на константу. Тогда мы можем потребовать, чтобы значение вот этой функции (скалярное произведение w на x минус w0) на крайних объектах по модулю было равно 1. Ну то есть, для класса +1 было равно 1, а для класса −1 было равно −1. Ну это означает, что если мы просто домножим на yi-тое, то тогда значение будет просто 1. Итак, потребовав такой нормировки, мы можем подсчитать ширину разделяющей полосы. Для этого нам нужно просто взять направление, задающее нормаль к разделяющей поверхности, то есть w, и посмотреть проекцию вектора разности крайних элементов каждого класса на это направление. Найти проекцию совсем несложно, нужно просто взять скалярное произведение разностей x с плюсом и x с минусом на вектор w, отнормированный на свою длину. Ну давайте распишем это скалярное произведение. Тогда в числителе мы получим разность скалярных произведений вектора w на x с плюсом и вектора w на x с минусом. Но в то же время мы знаем, что для этих крайних объектов выполнено некоторое соотношение, получающееся из нормировки. Воспользуемся им и тогда получим в числителе (w0 + 1) и (w0 − 1). В итоге ширина разделяющей полосы получается 2 делить на длину вектора w. Что это значит? Ну это значит, что теперь мы можем записать задачу, которая будет описывать максимизацию ширины разделяющей полосы. Давайте минимизировать скалярное произведение вектора w на себя (как мы знаем, его длина — это просто корень из этой величины) при условии, что отступ на этом объекте должен быть больше либо равен 1. Это наше ограничение, которое обеспечивает значение отступа 1 на крайних объектах класса. Теперь нам нужно рассмотреть случай линейно неразделимой выборки. Действительно, если до этого мы могли потребовать, чтобы отступ был больше либо равен 1, а 1 — это положительная величина (а отступ положителен тогда, когда классификация верная), то теперь, в случае линейно неразделимой выборки, у нас в любом случае будут объекты, которые неверно отнесены к классу нашим классификатором. Это означает, что мы должны позволить нашему алгоритму ошибаться, то есть позволить допускать отступ, который будет не больше либо равен 1, ну а, например, больше либо равен (1 − ξi). ξi-тое будет ошибкой на i-том объекте. Но тогда нам нужно добавить в минимизируемую функцию штрафы за эти ошибки. Ведь если мы их не добавим, то можно будет делать классификатор с любым произвольным отступом. Итак, добавить ошибки можно очень просто. Просто давайте напишем к исходному функционалу плюс константа умножить на сумму этих ошибок. Также обратите внимание: у нас появилась 1/2 перед скалярным произведением вектора w на себя, ну это просто из соображений удобства для дифференцирования. По сути, это ни на что не влияет. Смотрите, как у нас изменилась наша оптимизационная задача. Теперь у нас есть и переменные w, и переменные ξ. При этом на переменной ξ есть два условия. Одно ограничивает снизу отступ, а другое условие — это то, что ξi больше либо равны нулю. Ну действительно, зачем нам ошибки, которые будут отрицательны? Это просто означает, что где-то мы смогли сделать отступ положительным, но это и так все будет нормально получаться, даже если мы возьмем неотрицательные ξi-тые. Ну и теперь мы получили оптимизационную задачу для метода опорных векторов, ну или, проще говоря, SVM. Возникает вопрос: при чем же здесь линейный классификатор в привычном нам виде? Давайте посмотрим на эту задачу внимательнее. Действительно, мы хотим минимизировать сумму ξi-тых, при том, что ξi-тые должны быть больше либо равны 0 и больше либо равны единице минус отступ. Что это означает? Это означает, что ξi-тое будет уж во всяком случае больше либо равно максимума из двух величин (0 и единица минус отступ), и, с другой стороны, так как сумма ξi-тых минимизируется, то ξi-тая будет в точности равна этой величине, то есть максимуму из 0 и (1 − Mi). В таком случае мы можем просто подставить эти ξi-тые в нашу оптимизационную задачу, ну а именно в первое выражение. И тогда мы получим безусловную оптимизационную задачу в SVM, то есть задачу оптимизации без дополнительных условий. Смотрите, здесь уже четко прослеживается и функция потерь (она кусочно-линейная или, как говорят в англоязычной литературе, hinge loss), и регуляризатор — обычный L2-регуляризатор. Итак, метод опорных веторов — это просто линейный классификатор с кусочно-линейной функцией потерь и L2-регуляризатором. Придуман метод был из соображений максимизации зазора между классами. В случае линейно разделимой выборки это означает просто максимизацию ширины разделяющей полосы. А в случае линейно неразделимой выборки просто добавляется возможность попадания объектов в полосу и штрафы за эти попадания.

[ЗАСТАВКА] Пока ансамбли решающих деревьев не набрали своей популярности, очень часто люди использовали SVM даже в тех задачах, где разделяющая поверхность была совсем не похожа на линейную. Это получалось делать с помощью ядер. В основе лежит очень простая и очень красивая идея: если в каком-то исходном пространстве признаков классы не являются линейно разделимыми, то может быть можно отобразить это пространство признаков в какое-то новое, в котором классы будут линейно разделимы. Но если подумать, нам даже необязательно делать это отображение явно, потому что в SVM везде фигурирует только скалярное произведение вектора весов на вектор признаков. Отсюда возникает мысль — давайте вместо этого скалярного произведения использовать какую-нибудь другую, возможно, нелинейную симметричную функцию и таким образом получать нелинейную разделяющую поверхность. Эта идея называется в англоязычной литературе kernel trick. Давайте посмотрим на некоторые наиболее частые примеры ядер. Ну во-первых, можно взять линейное ядро, которое просто совпадает со скалярным произведением. Случай может показаться тривиальным, но его не стоит выкидывать из рассмотрения, потому что в некоторых ситуациях линейное ядро — это самый лучший выбор, ну например, если вы решаете задачу, связанную с классификацией текстов. Очень часто это именно так. Другой пример — это полиномиальное ядро. Полиномиальное ядро отображает исходное пространство признаков в пространство, в котором добавляются разные произведения признаков и степени признаков, и таким образом позволяет разделять классы, которые не были линейно разделимы в исходной выборке. И другое часто используемое ядро — это радиальное ядро. Как вы видите, оно понятным образом выражается через евклидово расстояние, поэтому наследует все проблемы метрических алгоритмов. Оно будет подвержено в некоторой степени проклятию размерности, в том числе поэтому не очень хорошая идея использовать радиальное ядро, например, на текстах, где признаков действительно очень-очень много. Но так или иначе, оно позволяет строить очень сложные границы классов, потому что если задуматься о том, в какое пространство здесь происходит отображение, можно сообразить, что пространство получается бесконечно мерным. Ну чтобы получить какое-то интуитивное доказательство этого факта, можно, например, расписать экспоненту в бесконечный ряд. SVM реализован в различных библиотеках, и в различных библиотеках есть поддержка ядер или неподдержка, ну то есть только линейное ядро. Например, в библиотеке LibSVM можно выбирать ядра, а в библиотеке LibLinear действительно только линейное ядро. Но это сделано осмысленно, именно для того чтобы этот конкретный случай с линейным ядром хорошо оптимизировать. Поэтому если вам на практике потребуется применить SVM с линейным ядром, лучше использовать LibLinear, а не LibSVM с указанием, что ядро должно быть линейным. В Scikit-learn мы видим просто обертку над LibSVM и LibLinear. Поэтому если вы хотите решить задачу с линейным ядром, опять-таки лучше использовать не SVC с линейным ядром, а LinearSVC. Также SVM можно найти в библиотеке Vowpal Wabbit, в которой по факту реализованы просто различные линейные классификаторы, и разумеется, там SVM только с линейным ядром, и представлен он просто в виде линейного классификатора с какой-то функцией потерь и каким-то регуляризатором. Ну то есть с функцией потерь hinge loss, ну или кусочно-линейной функцией потерь, и L2-регуляризатором. Итак, мы познакомились с вами с ядрами. Ядра до какого-то времени очень часто использовались в SVM. Также мы с вами рассмотрели линейное, полиномиальное и радиальное ядро и обсудили библиотеки, в которых SVM реализованы.