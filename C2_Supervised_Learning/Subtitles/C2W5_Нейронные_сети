[БЕЗ_ЗВУКА] В этом уроке речь пойдет о нейронных сетях вообще и в частности об однослойной нейронной сети. Нейронная сеть — это универсальная модель, решающая широкий класс задач регрессии и классификации. Задана выборка, множество пар «объект – ответ». Объект — это множество признаков в d-мерном пространстве, вектор, а ответ зависимой переменной y — это скаляр. Требуется построить аппроксимирующую поверхность, которая приближает ответы описаниями объектов. Рассмотрим задачу регрессии. На рисунке показана поверхность, которая оценивает риск в задаче страхования и финансов. По оси абсцисс и ординат — время до страхового случая и цена страховки. По оси аппликат — ожидаемый риск. Наша поверхность — нейронная сеть — аппроксимирует исторический риск y в точках x. Другие задачи регрессии: x — вектор исторических цен потребления электроэнергии, y — цена электроэнергии в следующий час; x — векторизованный снимок поверхности земли со спутника, y — объем зеленых насаждений; x — история продаж товара, y — уровень потребительского спроса. Рассмотрим задачу классификации. В этой задаче зависимая переменная y принадлежит конечному множеству и называется меткой класса. Требуется построить разделяющую поверхность, которая отделяет объекты одного класса от объектов другого класса таким образом, что при значении функции разделяющей поверхности большей нуля мы бы аппроксимировали объекты класса 1, а при меньшей нуля — объекты класса −1. На графике по осям показаны значения признаков x1 и x2, а значения функции a или значение метки класса y оно показано на нас в третьем измерении. И мы видим, толстая линия отделяет объекты синего класса от объектов оранжевого класса и как раз это и есть наша разделяющая поверхность a(x). Примеры задач классификации: x — временной ряд акселерометра мобильного телефона, y — вид физической активности; x — страница документа, y — релевантность этого документа по поисковому запросу; x — нотная запись в музыкальном произведении, y — это следующая нота, которая предполагается к проигрыванию. Что такое универсальная модель? Мы считаем нейронную сеть универсальной моделью, потому что она может аппроксимировать любые поверхности. В 1957 году Андрей Николаевич Колмогоров сформулировал следующую теорему: каждая непрерывная функция a(x), заданная на единичном кубе в d-мерном пространстве, представима в виде суперпозиции суммы функции σ от суммы функции f, где x — это наш вектор описания объекта в d-мерном пространстве. При этом функции σ и f должны быть непрерывны, и f не зависит от выбора a, что дает нам возможность конструировать различные нейронные сети. Иначе, другими словами, функцию от многих аргументов можно представить в виде суперпозиции многих функций от одного аргумента. Что такое единичный куб d-мерного пространства? Этот куб включает наши измерения, элементы выборки, которые аппроксимирует наша нейронная сеть. Если измерения — элементы вектора x — сделаны в различных шкалах, в разных шкалах (например, килограммы, амперы, секунды), то их следует обезразмерить. Например, отобразить измерения каждого из признаков в отрезок [0, 1]. Важно понимать, что Колмогоров не указал, какими именно должны быть функции σ и f, и это оставляет проблему конструирования нейронной сети очень острой и сложной. Рассмотрим однослойную сеть как единственный нейрон. Однослойная нейронная сеть или нейрон — это комбинация весов признаков или скалярное произведение объекта на вектор весов. σ — это наша функция активации, какая-то непрерывная монотонная функция, желательно дифференцируемая; w — это вектор параметров (или весов) признаков; x — это объект, это вектор с присоединенной единичкой, которая соответствует специальному весу w0, а этот вес регулирует положение нашей аппроксимирующей функции a относительно оси y. Нейрон можно графически представить в виде двудольного графа, в котором листья — это независимые переменные, а вершинка — это линейная комбинация и наша функция активации. Рассмотрим два примера использования нейронной сети как линейной модели. Первый пример — это задача регрессии, и в этой задаче функция активации тождественна алгебраической единице: что на входе, то и на выходе, и наша функция является линейной комбинацией значений признаков. Вторая задача — это задача классификации, в ней функция активации, пороговая функция сигнум от линейной комбинации. И мы говорим, что мы считаем объект крестиком, если функция, значение функции больше нуля, в противном случае, если она меньше нуля, то мы считаем объект ноликом. Однослойная нейронная сеть является моделью логистической регрессии, если мы используем сигмоидную функцию активации 1 поделить на 1 плюс экспонента от отрицательного скалярного произведения. Эта функция определяет вероятность принадлежности некоторого заданного объекта x к классу y равным единице при заданных параметрах w. На графике по осям абсцисс и ординат отложены значения признаков, точки — это объекты, y — это наши метки классов, и нашу выборку аппроксимирует как раз вот эта функция — сигмоидная функция активации от скалярного произведения. Для случая нескольких классов используется функция активации softmax. При этом сеть состоит из K одинаковых нейронов, и каждый нейрон вычисляет вероятность принадлежности к своему классу, а вся сеть вычисляет вероятность принадлежности объекта x к различным K классам одновременно. Какие есть еще виды функции активации? Гиперболический тангенс — это обобщение пороговой функции сигнум, которое может быть продифференцировано. Более мягкий вариант гиперболического тангенса — это функция активации softsign, которая сходится как и гиперболический тангенс к ±1, но данная функция она сходится медленнее, чем гиперболический тангенс. В сетях глубокого обучения, в более сложных нейронных сетях используются функция выпрямитель ReLu, работает как диод в радиоэлектронике, и эта функция она не дифференцируема, но имеет дифференцируемое приближение ln(1 + exp(x)). Итого: нейронная сеть — это универсальная модель, предназначенная для решения широкого класса задач регрессии и классификации, то есть для обучения с учителем. Нейрон, или однослойная нейронная сеть — это функция активации от линейной комбинации признаков объекта. При построении нейронной сети используются различные функции активации. Дальше двуслойные и многослойные нейронные сети.

[БЕЗ_ЗВУКА] В этом уроке мы обсудим многослойную нейронную сеть и различные функции ошибок. Однослойные нейронные сети, которые мы обсуждали ранее, применимы только для линейно разделимых выборок. Например, для конфигурации, представленной на графике, невозможно найти такую плоскость, которая разделяет крестики от ноликов. А для конкретного разделения этой выборки лучше всего подходит кривая линия. Рассмотрим двухслойную нейронную сеть. Двухслойная нейронная сеть — это линейная комбинация нейронов или однослойных нейронных сетей. Каждый нейрон — это линейная комбинация, которая представлена внутри внутренних скобок, и таких нейронов здесь D (большое). Внешние скобки — это как раз линейные комбинации или веса этих нейронов — нейронов первого слоя. σ со значком 2 — это функция активации второго слоя нейронной сети. Или то же самое в векторных обозначениях: двухслойная нейронная сеть — это следующая суперпозиция: функция активации σ со значком 2 (функция активации второго слоя), аргументом этой функции являются веса (вектор весов второго слоя), умноженные скалярно на сигмы функции активации первого слоя, а аргументом каждого элемента вектора σ первого слоя является скалярное произведение, то есть один из D (большое) нейронов первого слоя. Так как нейронная сеть — это параметрическое семейство функций, то вектор параметров нейронной сети получается соединением всех параметров нейронной сети на всех слоях. Двухслойную нейронную сеть можно представить в виде двух двудольных направленных графов, где исходящая вершина графа связана со всеми входящими вершинами. Этот граф можно дальше продолжать вправо для получения многослойной нейронной сети. В 1991 году Курт Хорник сформулировал Универсальную теорему аппроксимации, которая говорит, что для любой непрерывной функции найдется нейронная сеть a с линейным выходом, которая аппроксимирует эту функцию с заданной точностью. Теорема выполняется для различных функций активации, в частности для функции сигмоид и функции гиперболический тангенс. Для получения же заданной точности необходимо, кроме построения конфигурации нейронной сети, определить ее оптимальные параметры. Рассмотрим проблему качества аппроксимации с помощью следующего графика. На нем по осям абсцисс и ординат отложено значение признаков: x1 и x2. Объекты — синими и оранжевыми кружочками. Искомая неизвестная нам разделяющая поверхность показана пунктирной линией. Текущее значение разделяющей поверхности a, которая зависит от наших объектов x и от вектора параметров w, показана жирной линией, а несколько объектов попали в область, которая принадлежит другому классу вследствие случайной природы выборки. Тем не менее именно вот эту разделяющую поверхность мы называем оптимальной. Сравним ее с другой разделяющей поверхностью. В данном случае веса в той же самой нейросети настроены таким образом, что разделяют текущую выборку корректно, то есть все объекты принадлежат областям своего класса, но при небольшом изменении состава выборки разделение будет неверным и ошибка аппроксимации будет больше. Сеть, которая корректно аппроксимирует текущую обучающую выборку, но плохо аппроксимирует контрольную или какую-то другую выборку той же природы, называется переобученной. Сформулируем задачу нахождения оптимальных параметров нейросети как задачу оптимизации. Введем функцию ошибки Q, которая зависит от параметров. Также она, естественно, зависит от состава нашей выборки и от конфигурации нейросети. Какие есть функции ошибки? Функция ошибки для задачи регрессии называется функция «галочка», или сумма моделей разности между восстановленным значением и фактическим значением зависимой переменной y. Такая функция используется при решении прикладных задач в различных областях, например в финансах или при выполнении технических, физических, химических измерений. Это абсолютная разность между фактом и восстановленным значением. Но эта функция не дифференцируема. Есть дифференцируемая функция ошибки — это сумма квадратов разности между восстановленным значением и фактическим измерением. Для решения задачи классификации важно знать суммарное число несовпадений между восстановленными метками классов и фактическими метками классов. Такая функция ошибки называется «0–1 loss», и здесь квадратными скобками в этом выражении обозначена индикаторная функция. Она возвращает единичку, если выражение истинно, и нолик, если выражение внутри этих скобок ложно. Дифференцируемая функция ошибки в задаче классификации — это функция кросс-энтропия — функция наибольшего правдоподобия в задаче логистической регрессии, которую мы рассматривали ранее. Все 3 функции показаны на графике. В случае задачи регрессии мы видим «галочку» и аппроксимирующую ее непрерывную дифференцируемую функцию параболу (квадрат). В случае пороговой функции «0–1 loss» мы видим аппроксимирующую ее кросс-энтропийную функцию. Итак, с помощью многослойной нейронной сети можно получить аппроксимацию сколь угодно высокой точности. Для этого надо построить оптимальную структуру нейросети и оптимизировать ее параметры. Параметры оптимизируются с помощью минимизации функции ошибки. Существуют дифференцируемые функции ошибки для оптимизации ее параметров. Далее мы рассмотрим различные алгоритмы оптимизации параметров нейронной сети.

[ЗАСТАВКА] В этом уроке мы поговорим об алгоритмах оптимизации параметров нейронной сети. Задача оптимизации параметров — это задача минимизации функции ошибки. Функция ошибка... функция ошибки зависит от выборки, от структуры нейросети, то есть числа слоев, нейронов, видов функций активации, и от значения вектора параметров. На графике показана функция ошибки для двух параметров: по оси абсцисс и ординат отложено значение этих параметров, а по оси аппликат — значение функции ошибки. Функция ошибки может иметь значительное число локальных минимумов, которые даже в пространстве малой размерности нелегко обнаружить. Есть два типа алгоритмов: алгоритмы стохастической оптимизации и алгоритмы градиентного спуска. В случае стохастической оптимизации мы случайно перебираем решения вектора параметров w там, где они могут доставить минимум функции ошибки, например, случайным перебором или с помощью генетического алгоритма, который последовательно модифицирует значения вектора параметров. Или с помощью моделируемого отжига, который изменяет значения параметров по расписанию и сначала ищет минимум в широкой области, а потом сужает область поиска до нахождения глобального минимума. Алгоритм локальной аппроксимации функции ошибки минимизирует не саму функцию ошибки, а функцию, которая ее аппроксимирует в окрестности точки w. Например, удобно аппроксимировать с помощью квадратичной функции. В многомерном случае эта квадратичная форма... в многомерном случае эта квадратичная функция имеет вид квадратичной формы. На каждом шаге алгоритма мы находим минимум такой функции и сдвигаем окрестность, в которой продолжаем аппроксимацию и поиск минимума исходной функции. Алгоритм градиентного спуска предполагает, что функция ошибки дифференцируема, то есть она квадратичная или кросс-энтропийная, и, задавая значения параметров w, можно вычислить значение функции ошибки и ее градиент. На рисунке по оси абсцисс и ординат показано значение параметров — у нас опять двумерное пространство параметров, по оси аппликат показано значение функции ошибки. И требуется, отправляясь из исходной точки wC, спуститься к точке минимума wB, шагая в направлении от градиента — ∇ с индексом w Q. При этом, конечно же, надо помнить, что точка wB может оказаться как глобальным минимумом, так и локальным минимумом, как, например, точка w с индексом A. Градиентный спуск — это пошаговая процедура нахождения параметров. У нас каждый следующий шаг зависит от предыдущего и от суммы градиентов функции ошибки на одном-единственном объекте. Обозначим эту ошибку буквой φ, обозначим ее частную производную по параметрам ∇wφ. α — это величина шага, который мы делаем, и k — это номер итерации итерационной процедуры. Шагаем до тех пор, пока значение функции ошибки не стабилизируется. Вариант градиентного спуска — это стохастический градиентный спуск. Нам совсем не обязательно использовать всю выборку для вычисления градиента. Предлагается направление градиента вычислить для случайно выбранного объекта x, и, таким образом, наш градиентный спуск будет выглядеть следующим образом: wk +1 — это wk на предыдущем шаге + α — длина шага, ∇wφ — это градиент на единственном объекте. И данная процедура использует случайно переупорядоченные объекты и продолжается до стабилизации функции ошибки. Очень популярный и быстрый алгоритм — это алгоритм обратного распространения ошибки backpropagation, или backprop. Для каждого нейрона сети и одного объекта выборки x вычисляется значение выхода нейрона — оно зависит от выходов предыдущих нейронов, и это вычисление называется прямым распространением. И производную ошибки по значению нейрона и по его параметрам — эта производная зависит от значений последующих нейронов и называется обратным распространением. Существует проблема затухания градиента при обратном распространении: при больших значениях входов нейрона значения производной функции активации стремятся к 0, например, для сигмоидной функции и для гиперболического тангенса, как это показано на графиках. При этом градиент функции ошибки не распространяется по слоям. Преимущества и недостатки обратного распространения функции ошибки. Преимущества: градиент вычисляется за время, сравнимое с вычислением всей сети, алгоритм подходит для многих дифференцируемых функций активации и функции ошибки, необязательно использовать всю выборку. Но возможна медленная сходимость к решению, решение может быть в локальном минимуме и возможно переобучение сети. Итак, для оптимизации используются как стохастические, так и градиентные методы. Стохастические методы требуют многократного угадывания вектора параметров и могут работать довольно долго. Градиентные методы требуют дифференцирования функции ошибки, но могут застревать в локальных минимумах, и поэтому выбор метода оптимизации остается прикладным искусством и, конечно же, зависит от характера решаемой задачи. Далее мы поговорим о регуляризации и прореживании нейронной сети.

[ЗАСТАВКА] В этом уроке обсудим способы регуляризации и прореживания нейронной сети. Для того чтобы избежать переобучения, модифицируем задачу оптимизации и добавим к функции ошибки штраф за большие значения параметров. Чем меньше коэффициент регуляризации τ, коэффициент перед суммой значений параметров, тем точнее функция описывает выборку. Коэффициент регуляризации контролирует жесткость ограничений параметров. На рисунке слева коэффициент регуляризации имеет самое большое значение: у нас область поиска решения в пространстве параметров мала, но смещенность параметра от минимального значения функции ошибки велика. Справа мы видим, что область поиска оптимальных параметров велика, смещенности нет, но, возможно, наша сеть переобучена. В центре мы видим компромиссный вариант между смещенностью и областью поиска оптимальных значений параметров. То же самое можно показать с помощью следующего графика, здесь по оси абсцисс отложен коэффициент регуляризации, а по оси ординат — значения параметров. Видно, что регуляризация не снижает число параметров и не упрощает структуру сети. Более того, при увеличении коэффициента τ параметры перестают изменяться. Для снижения числа параметров предлагается исключить некоторые нейроны или связи. Таким образом, наш двудольный граф перестанет быть полносвязным. Принцип исключения следующий: если функция ошибки не изменяется, то сеть можно упрощать и дальше. Здесь на графике по оси абсцисс показано число удаленных связей или параметров, а по оси ординат показано значение функции ошибки. Видно, что мы удалили практически все параметры, и только в конце значение функции ошибки начало изменяться. Какие есть стратегии прореживания параметров в сети? Параметр можно удалить, если его вклад в нейрон равен нулю, то есть он сам имеет значение, близкое к нулю, дальше, если его значение сильно меняется при изменении выборки (у него большая дисперсия, то есть он реагирует на шум в данных) и если его удаление меньше всего влияет на изменение значения функции ошибки. Рассмотрим последний вариант подробней. Метод удаления параметров называется «метод оптимального прореживания» или «метод оптимального разрушения мозга». Разложим функцию ошибки в ряд Тейлора. При этом первое слагаемое при фиксированных параметрах будет константой, второе слагаемое будет нулем, потому что мы считаем, что мы находимся в оптимуме функции ошибки, третье слагаемое будем минимизировать, а четвертое слагаемое проигнорируем. Найдем такой параметр нейронной сети, для которого его изменение равно его значению, или по-другому: несмотря на его изменение, он имеет нулевой вклад в нейрон. Этот параметр имеет минимальное значение функции выпуклости, которая вычисляется как квадрат его величины, отнесенный к соответствующему диагональному элементу обратной матрицы Гессе — матрицы вторых производных, которые соответствуют третьему слагаемому нашего разложения. Каким образом строится нейронная сеть? Нейронная сеть, она используется в двух режимах: во-первых, режим обучения — при этом мы задаем структуру нейронной сети и оптимизируем ее параметры, и в режиме эксплуатации — при этом мы вычисляем значение, которое аппроксимирует нашу выборку при фиксированных параметрах. Для того чтобы задать нейронную сеть, требуется задать число слоев, число нейронов в каждом слое, тип функции активации в каждом слое, тип функции ошибки. И желательно, чтобы подготовленная выборка не содержала пропусков, а признаки были отнормированы. При обучении желательно следить за стабилизацией параметров сети и за скоростью обучения сети. По скорости обучения можно судить о соответствии выборки и нейронной сети. Если нейронная сеть будет слишком сложна, то она быстро переобучится, как, например, показывает желтая линия. Если выборка будет сложна или сильно зашумлена для нейронной сети, то нейронная сеть будет обучаться медленно, как показывает синяя линия. Если выборка по сложности соответствует нейронной сети, то, скорее всего, мы увидим красную линию — хорошую скорость обучения. Важно также следить за разницей между значениями функции ошибки на обучении и на контроле. Эта разница не должна быть существенной. Если она велика, то это значит, что нейронная сеть переобучилась и следует изменить ее сложность. Итак, регуляризация используется для снижения переобученности сети путем загрубления ее параметров. Оптимальное прореживание упрощает структуру сети, удаляя лишние параметры. Структура нейронной сети существенно зависит от решаемой прикладной задачи. Ее построение и оптимизация выполняются, как правило, экспериментальным путем.