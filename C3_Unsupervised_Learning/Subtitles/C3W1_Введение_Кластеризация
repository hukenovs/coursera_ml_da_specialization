ЗАСТАВКА Привет. Мы с вами приступаем к курсу «Поиск структуры данных» и начинаем знакомиться с задачами машинного обучения, в которых нет обучающей выборки в привычном для нас понимании. То есть у нас не будет примеров с известными ответами, на которых можно обучиться. Но при этом нам нужно будет как-то структурировать данные. Первая задача, которой мы займемся — это задача кластеризации. В ней нужно разбить всё множество примеров на группы похожих объектов, называемые кластерами. Мы посвятим задаче кластеризации два урока. В первом уроке мы разберемся с постановкой задачи, примерами использования кластеризации, с популярными алгоритмами решения этой задачи и даже попробуем применить эти алгоритмы на практике, после чего осмыслим, в каких ситуациях лучше использовать какие алгоритмы. Конечно, обзорным знакомством с алгоритмами все не закончится. Во втором уроке мы поговорим об алгоритмах подробнее, а также поговорим о метриках качества в задачах кластеризации и о некоторых практических трюках.

[ЗАСТАВКА] В этом видео мы начнём с вами знакомиться с задачей кластеризации. Что у нас было раньше? Раньше мы разбирались с задачами обучения на размеченных данных. То есть у нас была обучающая выборка, в которой были объекты — x1, ..., xl. Ну то есть векторы признаков объектов из обучающей выборки. И известны ответы на этих объектах — y1, ..., yl. Кроме того, у нас была тестовая выборка, на которой нам нужно было эти ответы спрогнозировать. В случае задачи регрессии нужно было прогнозировать какие‐то действительные величины. В случае задачи классификации нужно было прогнозировать метки классов. Ну то есть относить объект к тому ли иному классу, а классов у нас кое‐то конечное число. Фактически мы занимались восстановлением некоторого отображения из векторов признаков x в какие‐то метки или какие‐то действительные числа. То есть мы пытались построить какую‐то функцию a(x), которая бы хорошо приближала это отображение. Ну то есть чтобы a(x) было приблизительно равно y. Ну что значит — приблизительно равно? В случае задачи регрессии это означает, что отклонение должно быть не очень большое. А в случае задачи классификации это означает, что ответ должен очень часто совпадать с правильным. Теперь же у нас задача меняется. У нас есть обучающая выборка, в которой есть только объекты — x1, ..., xl. А вот ответов теперь нет. То есть эта обучающая выборка также является и тестовой. И нам нужно поставить метки y1, ..., yl, так чтобы объекты с одной и той же меткой были похожи друг на друга, а объекты с разными метками были не очень похожи друг на друга. Ну что это значит? Это значит просто то, что мы хотим в пространстве признаков выделить какие‐то сгустки объектов, какие‐то группы. И на эти группы все объекты и разбить. Получается, что здесь нам тоже нужно восстановить какое‐то отображение. Но дело в том, что раньше у нас были примеры значений иксов и игреков. А теперь у нас таких примеров нет. Получается, теперь мы не можем потребовать, чтобы наше восстановленное отображение приближало какое‐то, известное нам в некоторых точках. Но мы можем потребовать, чтобы оно обладало какими‐то свойствами. Ну, например, теми самыми свойствами, которые мы только что обсудили: похожие объекты должны отображаться в одну метку, а непохожие — в разные метки. Для этого нам нужно придумать, как формализовать это свойство, ввести какие‐нибудь метрики, которые будут измерять качество нашей кластеризации. Ну, во‐первых, мы можем посмотреть на то, какие точки попадают в один кластер. И посмотреть, какое среднее расстояние между точками, которые попадают в один кластер. Понятно, что такое расстояние нам будет хотеться минимизировать. Чем ближе точки в одном кластере, тем больше они друг на друга похожи, если у нас адекватное пространство признаков. С другой стороны, мы можем хотеть, чтобы разные кластеры отличались друг от друга. То есть мы можем посмотреть на среднее расстояние между точками, принадлежащими к разным кластерам, и пытаться это расстояние максимизировать. Понятно, что, максимизируя эту метрику или минимизируя предыдущую, мы не учитываем другую метрику, поэтому можно было бы их объединить в одну, ну, например, поделить одну на другую. Ну, например, первую на вторую и минимизировать. Итак, в этом видео мы поговорили об отличиях задачи кластеризации от задачи обучения на размеченных данных. Мы поставили задачу кластеризации и поговорили о простых способах оценить качество решения задачи кластеризации. В следующем видео мы поговорим немного о разных постановках задачи кластеризации, о том, как разные задачи кластеризации могут отличаться друг от друга, и рассмотрим несколько частных применений.

В этом видео мы поговорим о примерах задач кластеризации. Дело в том, что каждые данные по-своему особенны, так же, как и каждая задача кластеризации. В разных задачах могут быть отличия по форме кластеров, которые нужно выделять, по необходимости делать кластеры вложенными друг в друга и выстраивать какую-то иерархическую структуру кластеров, по размеру кластеров, по тому, основная задача кластеризации или побочная... по тому, жесткая или мягкая кластеризация нужна. Ну, то есть, мы можем отнести объект сразу к нескольким кластерам или только к одному. В задачах с разными особенностями могут быть уместны разные методы, именно поэтому методов кластеризации так много. Ну, давайте сначала обсудим форму кластеров. Может быть совсем простой и безоблачный случай, когда кластеры представляют собой простые сгустки точек, имеющие какую-то понятную форму, ну, например, шарообразную, и их довольно просто выделять. Может быть ситуация чуть ближе к реальности, когда форма кластеров действительно какая-то необычная, но в то же время сгустки четко выделяются. Может быть ситуация, когда есть точки, находящиеся где-то между кластерами, и даже непонятно, к какому кластеру их правильно отнести. А могут быть ситуации довольно необычных структур, ну, например, каких-то вытянутых лент кластеров, которые отличаются тем, что мы можем найти пару точек из разных кластеров, которые будут ближе, чем какая-нибудь другая пара точек из одного кластера. Например, ленты могут вызвать у нас желание сначала объединять в кластер точки, которые находятся рядом друг с другом, а потом уже к ним добавлять другие точки кластера, постепенно добавляя в кластер всю ленту. Но если мы будем действовать по такой стратегии, то мы столкнемся с проблемой, если кластеры плавно перетекают друг в друга. Кроме того, не стоит забывать, что кластеры могут быть какими-то очень странными множествами, которые образованы по какому-то неизвестному нам правилу, но в то же время существующему правилу. Ну, и, кроме того, кластеры могут пересекаться. И тогда совсем уж сложно понять, какой объект к какому кластеру относится, в каких-то отдельных областях. Ну, и в конце концов кластеров может попросту не быть. Теперь о вложенности кластеров. Нам может захотеться, чтобы в какой-то один большой кластер входили кластеры поменьше. Ну, например, если мы кластеризуем тексты статей из "хабрахабр", кластер "IT" может включать в себя кластер "алгоритмы", в котором внутри будут кластер "методы машинного обучения" и "алгоритмы и структуры данных". И там, и там – про алгоритмы, но немножко про разные. Также может различаться размер кластеров. Давайте продолжим тему кластеризации текстов на примере кластеризации новостей по содержанию. Первая постановка может быть такой: в один кластер должны попадать новости на одну тему. Ну, например, в одном кластере могут оказаться все новости про спорт. Другая постановка: в одни кластер должны попадать все новости о каком-то большом событии, ну, например, об Олимпиаде в Сочи. Третья постановка: в один кластер должны попадать тексты об одной и той же новости, ну, например, открытие Олимпиады в Сочи. Задача кластеризации новостей в каком-то смысле подразумевала, что задача кластеризации здесь основная. Ну, то есть мы не пытаемся использовать результат задачи кластеризации для какой-то другой задачи машинного обучения. Могут быть и промежуточные варианты. Ну, например, если мы решаем задачу сегментации целевой аудитории компании, для того чтобы в разных группах использовать разные маркетинговые стратегии или предлагать разные услуги, то может быть мы захотим решать задачу рекомендации разных предложений после того, как решим задачу кластеризации. И тогда задача кластеризации станет в некотором смысле побочной. А может быть мы просто хотим посмотреть на кластеры, и тогда задача будет основной. Ну и совсем уж яркий пример – это задача распознавания. Часто бывает, что нужно уметь распознавать символ, независимо от того, каким шрифтом он написан: курсивом, полужирным... Как пишутся отдельные элементы символа... И тогда может быть полезно кластеризовать разные написания символа и использовать это для лучшего решения задачи классификации символов, ну, то есть распознавания. Также кластеризация может быть жесткой или мягкой. Ну, например, если мы пытаемся кластеризовать тексты по темам и у нас есть темы "финансы", "анализ данных" и тема непосредственно про кластеризацию, то статья про кластеризацию, использованную в какой-нибудь задаче из области финансов, может быть отнесена напрямую к кластеру "кластеризация" и все. А может быть отнесена с весом 0,2 к кластеру "финансы", с весом 0,3 – к кластеру "анализ данных" и с весом 0,5 – к кластеру "кластеризация". Поведем итог. Задачи кластеризации могут различаться формой кластеров, которые нужно выделять, необходимостью строить иерархическую структуру из кластеров, размеров кластеров... Ну, то есть насколько узко мы понимаем тот факт, что объекты попадают в один кластер или в разные. Тем решаем мы конечную задачу или лишь вспомогательную, для того чтобы затем решить задачу классификации, или регрессии, или еще какую-нибудь. И тем, жесткая кластеризация или мягкая. То есть относим мы объект к одному кластеру или сразу к нескольким с каким-то весом.

[ТИШИНА] Должно быть, вы уже думаете, ну, сколько можно обсуждать задачу, давайте обсудим хоть какой-нибудь алгоритм, который ее решает. В этом видео именно этим мы и займемся. Мы рассмотрим сразу несколько алгоритмов, конечно, достаточно обзорно, и увидим, как по-разному можно решать задачу кластеризации. Конечно, впоследствии мы поговорим об этих алгоритмах подробнее. Итак, мы уже знаем, что кластеры могут выглядеть совершенно по-разному. И универсального метода кластеризации, как следствие, мы не придумаем. При этом, разные методы тоже работают по-разному. Например, сейчас на слайде показаны результаты работы двух разных методов кластеризации, метода k средних и ЕМ-алгоритма, на модельном дата-сете Mouse dataset. Как вы видите, результаты действительно существенно отличаются. Ну, давайте начнем с простого метода, с метода k средних. Предполагается, что мы откуда-то знаем, сколько хотим сделать кластеров. Это и есть константа k, и изначально мы выбираем k случайных точек в качестве центров кластеров. После этого мы распределяем точки выборки по этим центрам, в зависимости от того, к какому центру точка ближе. Таким образом, мы получаем некоторое первое разбиение на кластеры. Но, наверняка, оно будет не очень хорошо. Поэтому мы можем взять и пересчитать центры просто как среднее арифметическое точек, попавших в один кластер. Пересчитав центры, мы можем увидеть, что теперь можем распределить точки заново, снова смотря на расстояние до центров. Мы можем продолжать этот алгоритм до тех пор, пока он не сойдется к какому-то довольно неплохому на вид решению. Этот алгоритм можно модифицировать, для того чтобы подбирать количество кластеров. Так можно придумать алгоритм Bisect k Means, который заключается в том, чтобы на каждом шаге делать 2 means, ну, то есть k means с k равным 2. После того, как мы сделали 2 means, мы смотрим на каждый получившийся кластер и запускаем 2 means в нем, если кластер все еще нужно дробить. Более сложный метод как раз тот, который лучше всего справлялся с задачей кластеризации на модельном dataset, показанном в начале видео, это EM-алгоритм. EM-алгоритм исходит из предположения, что у каждого кластера есть какая-то своя плотность, pj (x), из какого-то семейства, например, из нормального. И мы смотрим на плотность всех точек как на взвешенную сумму плотностей кластеров. Дальше мы последовательно повторяем E- и M-шаг. На E-шаге мы оцениваем некоторые вспомогательные переменные, зафиксировав которые, удобней максимизировать правдоподобие. А на M-шаге мы выполняем некоторые действия, чтобы оценить те переменные, которые нам нужно знать, то есть веса разных компонент pj и параметры распределения этих компонент. По сути, мы решаем задачу разделения смеси распределений. То есть мы считаем, что у нас есть смесь, ну, то есть взвешенная сумма плотностей, и нам нужно оценить веса и оценить параметры отдельных компонентов. Эту задачу можно было бы попробовать решать в лоб, просто выписав правдоподобие и подбирая параметры таким образом, чтобы правдоподобие было максимальным. Но оказывается, что это не очень удобно, и, если попытаться аналитически выписать решения этой задачи, скорее всего вы потерпите неудачу. А вот с помощью EM-алгоритма, повторяя E- и M-шаг, можно справиться довольно неплохо. Для примера рассмотрим простой иллюстративный случай, когда мы сгенерировали две компоненты из нормальных распределений на плоскости. Вот вы видите на слайде два сгустка, их можно проинтерпретировать как кластеры. И вот, что получается, если применить к этой выборке EM-алгоритм. Обратите внимание, так как мы предположили, что плотности нормальные, можно выписать EM-алгоритм более точно, то есть на M-шаге, там, где у нас решалась задача максимизации некоторой функции, можно выписать явные аналитические решения. Ну, если все эти формулы немножко ввергли вас в уныние, не расстраивайтесь, есть и более простые методы кластеризации, а о EM-алгоритме мы еще поговорим позднее. Например, есть методы, основанные на плотности точек. В основном они используют следующую идею. Давайте смотреть на окрестность точки, и, если в окрестности точки много других точек, будем считать, что это основная точка из внутренности кластера. Если в окрестности точки мало других точек, но есть уже какая-то другая основная точка, то будем называть эти точки пограничными, если же в окрестности точки вообще мало чего есть, эти точки будут шумовыми. Один из density-based методов, то есть методов, основанных на плотности точек, это DBSCAN. Он устроен следующим образом. Сначала мы помечаем все точки как основные, пограничные, шумовые, затем мы отбрасываем из рассмотрения шумовые точки, соединяем все основные точки, находящиеся на малом расстоянии друг от друга, дальше объединяем каждую группу соединенных основных точек в отдельный кластер и назначаем пограничную точку к тому кластеру, который больше представлен в ее окрестности основными точками. Можно придумать и другой подход к кластеризации, так называемую иерархическую кластеризацию, которая устроена следующим образом. Изначально будем считать, что кластеров у нас по числу объектов выборки, каждый объект это кластер из одного элемента. Дальше мы введем некоторое расстояние на кластерах и на каждом шаге будем объединять те кластеры, расстояние между которыми самое маленькое. Таким образом мы можем изобразить все эти объединения кластеров и получить дерево, вершины в котором являются кластерами. Это дерево можно обрезать на разной глубине, получая разное количество кластеров, не перезапуская алгоритм кластеризации заново, как это потребовалось бы в случае, если бы мы делали, ну, например, k means. Можно заметить, что это все напоминает ситуацию из биологии, когда мы пытаемся систематизировать виды и понять, как они произошли, и в какие группы они объединяются. Иерархические методы кластеризации позволяют строить довольно удобную визуализацию их работы, так называемые дендрограммы. Дендрограммы выглядят следующим образом. На одной оси отмечены точки, соответствующие объектам выборки, на другой оси мы откладываем расстояние между кластерами в момент слияния. Так, можно посмотреть, как это расстояние меняется в разных по счету слияниях, и выбрать то самое место, где мы начнем уже отсекать дендрограмму и решать, что вот, наши отдельные кластеры, которые мы будем выделять. Ну, то есть понимать, после какого слияния нам уже сливать кластеры неинтересно. Расстояние между кластерами можно вводить по-разному. Можно посчитать среднее расстояния между объектами кластеров, можно посчитать максимальное расстояние, можно минимальное, можно придумать еще много разных методов. Разные методы будут приводить к разным результатам, которые могут вас в разной степени устраивать. Теперь, когда мы уже знаем, что задачи кластеризации бывают разными, из прошлого видео, и из этого видео мы знаем, что есть много разных методов, можно попробовать систематизировать все. Ну, то есть можно подумать, какие из рассказанных методов к каким видам методов относятся. Ведь методы кластеризации могут различаться, во-первых, по структуре кластеров, они могут быть иерархическими, то есть сложенными кластерами, и плоскими. Иерархические, в свое время, делятся на агломеративные, то есть заключающиеся в том, что мы последовательно объединяем кластеры, и дивизионные, то есть заключающиеся в том, что мы все помещаем в один большой кластер и потом делим. Методы могут хорошо работать на разных формах кластеров. Подумайте, на каких формах какие методы будут работать лучше. И по присвоению объектов кластерам. Какие-то методы позволяют легко сделать жесткую или мягкую кластеризацию, а какие-то позволяют делать только жесткую. Подведем итог. В этом видео мы обзорно познакомились с несколькими методами кластеризации, с методом k средних, с EM-алгоритмом, с методами, основанными на плотности точек, то есть с density-based методами, и с иерархической кластеризацией. В дальнейшем мы еще поговорим об этих и других методах подробнее, и о том, какие методы лучше применять в каких-то конкретных ситуациях.

[НЕТ ЗВУКА] В этом видео мы разберем с вами пример, в котором попробуем использовать методы кластеризации на практике. Кластеризовать мы будем письма. Письма на разные темы. Для этого нам потребуется выборка 20newsgroups. Она может быть загружена функцией встроенной sklearn. Ну давайте посмотрим, какие темы там представлены. Тем, действительно, много. Ну и давайте не будем работать сразу со всеми, а для начала сделаем себе простую выборку, в которой будет всего три темы, очень сильно отличающихся друг от друга. Ну например, что-нибудь про Маки, про религию, в частности христианство, и про спорт, в частности хоккей. Ну казалось бы, что может быть более различным? Уж наверное здесь у нас все получится. Подготовим датасет и посмотрим, что в нем внутри. Как видите, письмо действительно вроде бы как про Мак, и, в общем-то, можно посмотреть на другие классы. Все классы здесь отображаются ноликами, единичками и двоечками, поэтому давайте попробуем понять, что значат эти единички и двоечки. Если мы посмотрим на последний элемент, у него класс 2, то увидим, что здесь, скорее всего, про христианство, а если посмотрим на предпоследний элемент, то увидим, что это, по ходу, переписка каких-то ребят из NASA, однако это переписка про хоккей. Ну и давайте посмотрим, сколько у нас вообще объектов в выборке. Не очень много, 1777. Ну и приступим к тому, чтобы подготовить какие-то признаки. Итак, давайте попробуем для начала просто частоты слов. Посмотрим на нашу матрицу. Матрица получилась количество объектов на 3767. Так вышло из-за того, что мы задали пороги для максимальной документной частоты слов и для минимальной. То есть мы в этой ситуации не хотели видеть слова, которые встречаются больше чем в 500 документах, ну для того, чтобы как-то более-менее удобно различать разные кластеры. Наверняка во всех кластерах есть слова from, subject и тому подобное. А с другой стороны, мы не хотели смотреть на слишком редкие слова, чтобы у нас не получалась слишком большая матрица. Ну и давайте попробуем воспользоваться ну например агломеративной кластеризацией, потому что в ней мы можем задать используемую метрику или функцию близости и, в частности, можем использовать косинусную меру, которая, казалось бы, так неплохо должна подходить к тексту. Давайте это все просто запустим. Обратите внимание: перед тем, как делать fit_predict, я преобразую матрицу к другому формату. Дело в том, что матрица после извлечения признаков получится в разреженном формате. И конечно, в таком формате с ней работать удобнее, ведь в ней много нулей. Зачем нам хранить все эти нули, если можно хранить только значения в тех местах, где не ноль? Но, к сожалению, реализация алгоритма не поддерживает разреженные матрицы, и поэтому приходится приводить к плотному виду и, естественно, расходовать очень много памяти. Посмотрим на результаты. Результаты выглядят более чем ужасно. Действительно, у нас везде практически кластер 0. Есть небольшие вкрапления кластера 1, кластера 2, но не это мы ожидали в этой задаче. Давайте попробуем все же вместо частот слов использовать взвешенные частоты с помощью tf-idf векторайзеров. Подробнее о разных признаках в анализе текстов мы еще поговорим в пятом курсе нашей специализации. Давайте это снова запустим. Давайте снова запустим обучаться. Что еще могло нас подвести? Может быть, мы использовали недостаточно жесткие параметры для отсечения. Может быть, недостаточно мягкие. Может быть, нужно увеличить параметр max document frequency, а может быть уменьшить. Это мы тоже можем попробовать. Тем временем у нас отработал алгоритм и мы можем снова посмотреть на результаты. Результаты по-прежнему не впечатляют. Давайте проверим наши гипотезы насчет уменьшения или увеличения параметров. Ну давайте попробуем смотреть только на те слова, которые встретились хотя бы в 100 документах. Если подумать, то в предположении, что наши кластеры примерно одинакового размера, каждый кластер будет больше 100 документов и, может быть, какие-то слова все же получится выцепить. Давайте сделаем минимальную документную частоту на 10, а 100, запустим это все, посмотрим на матрицу, увидим что теперь у нас всего 333 признака. На самом деле это, может быть, уже подозрительно, но выполним код и посмотрим на результаты. Конечно, при таком количестве признаков все отработает еще быстрее. Ну и здесь все совсем безнадежно. Смотрите, просто практически одни нули. Хорошо, можно попробовать изменить первый параметр. Давайте сделаем так, чтобы признаков все же было чуть побольше. Ну как вы видите, не сильно все поменялось. Давайте наконец убедимся, что эти признаки тоже не исправили всю ситуацию. Смотрим результат. Ну в какой-то степени, может быть, стало и получше, но сложно судить визуально. Давайте вернем все как было, потому что ничего лучше мы пока не сделали. [МОЛЧИТ] Тем временем можно подумать, что еще можно предпринять, чтобы как-то справиться с этой задачей. Ну, может быть, мы допустили где-нибудь ошибку, когда строили признаки. Может быть, стоит посмотреть на конкретные письма, на конкретные признаки и проверить, что все работает правильно. Давайте посмотрим на нулевой объект из выборки. И вот мы видим, что у него есть признак с индексом 877 и давайте для примера на него и посмотрим. Можно посмотреть, какие признаки у нас в принципе есть. Ну вот видно, что это разные слова. И видно, что есть много всякого мусора. Непонятно, мусор мешает нам сейчас или нет, но посмотрим сначала на вот этот признак. Этот признак соответствует вполне конкретному слову. Посмотрим на сам текст. Слово в нем есть, значит вроде бы все более-менее правильно. Тогда остается последняя страшная мысль. Видимо, у нас очень много всякого шума. У нас очень плохие признаки из вот этих всяких чисел и нужно как-то фильтровать признаки. Мы бы могли этим сейчас заняться, и тогда бы это видео длилось не 15 минут, а может быть 3 часа, но мы своевременно вспомним, что существуют и другие методы кластеризации, в частности, очень простой метод кластеризации k-средних. Давайте попробуем его. Итак, мы видим, что получились какие-то прогнозы. Да, обратите внимание, количество кластеров задано равным трем, и также я задал random_state для того, чтобы результаты были воспроизводимы. Это нужно исключительно для того, чтобы понимать, какие метки будут у кластеров для того, чтобы не переписывать код, который будет ниже. Ну давайте посмотрим, какие ответы должны были получиться. Смотрите, безумно похоже. Нули на том же месте, а вместо двойки хотелось бы поставить единицу, вместо единицы — двойку. Ну то есть получается, что мы просто не угадали с тем, как пометить кластер, но это не важно. Нам же важна суть. Но кластеризация похожа на правду. Давайте возьмем и сделаем отображение, в котором двойка перейдет в единицу, единица — в двойку, а нолик останется. И посмотрим, в какой доле случаев мы неверно угадываем кластер. Просто фантастика, 4,6 %. То есть мы практически с точностью 96 %, ну даже, точнее, 95 %, угадываем кластеры. Это просто потрясающе, при том что у нас не было обучающей выборки с ответами. Давайте для примера сравним это все с классификатором. Уж классификатор-то знает, какие ответы на обучающей выборке. И, если посмотреть на качество логистической регрессии в кросс-валидации, можно увидеть 98,5 %. То есть, в принципе, 95 % от 98 не сильно далеко ушло, и это по-прежнему удивительно. Ну наверное дело в том, что мы взяли очень простую выборку. Давайте попробуем что-то более сложное, например, на этот раз выберем три темы, которые близки друг к другу. Пусть они все будут про компьютеры, и сразу же попробуем на них применить метод K средних. [БЕЗ ЗВУКА] Конечно, здесь уже отличить тему будет сложнее, мы не ожидаем, что качество будет таким же. Более того, не очень понятно, какие числа куда отображать. Но, вообще, если присмотреться, можно увидеть, что, наверно, нолик в двойку, двойку в ноль и единицу в единицу. Хотя вот даже уже среди этих примеров мы видим ошибку. Ну давайте попробуем так сделать и посмотреть на качество. Смотрите, теперь уже ошибок 26 %. В принципе, тоже не очень много, потому что у нас даже не два кластера, а три кластера. И самое главное, наш алгоритм кластеризации совершенно не догадывается, по какому принципу мы делим тексты на группы, ведь это можно сделать кучей разных способов. И если мы хотим как-то передать это знание нашему алгоритму кластеризации, нужно просто выбирать правильным образом признаки. Вот, оказывается, те признаки, которые мы выбрали, достаточно близки к тому, что нам нужно. Давайте посмотрим на качество классификатора. Классификатор здесь уже существенно обгоняет, тут, конечно, качество не 70 %, а 91, даже почти 92. Но, оказывается, что и это качество можно чуточку улучшить, ведь у нас сейчас очень много признаков, можно попробовать уменьшить количество признаков, воспользовавшись сингулярным разложением матриц, то есть SVD. Мы уже знакомились с SVD в первом курсе и еще будем продолжать знакомство в этом курсе. Давайте сейчас просто воспользуемся им и попробуем оставить всего 1000 признаков. После этого нам опять же потребуется как-то сопоставить полученные кластеры с правильными ответами и вновь посмотреть на качество. Смотрите, качество стало 20 %. Всего 20 % ошибок! Удивительно! Кстати говоря, давайте попробуем оставить не 1000 компонент, а 200. Казалось бы, мы очень сильно понизим размерность пространства признаков, наверняка уже все сломается. Здесь уже реализована простенькая функция, которая просто выводит все результаты в зависимости от разных перестановок, то есть мы можем отображать двойку в некоторую переменную a, единичку в некоторую переменную b, нолик в некоторую переменную c. Здесь мы перебираем все возможные перестановки нолика, единички и двойки и выводим результаты. И, смотрите, самая лучшая перестановка дает опять-таки 20 % ошибок. То есть, мы оставили всего 200 признаков, 200 признаков из более чем трех тысяч, и по-прежнему качество у нас такое же. На самом деле не стоит быть слишком уж доверчивым к этому всему. Дело в том, что здесь я выбрал очень удачный random_state, а с другим random_state могут получится и совсем другие результаты. Давайте посмотрим. Ну вот, лучший результат 26 %. Подведем итог. Мы с вами смогли получить интерпретируемый результат кластеризации текстов. При этом мы это сделали как на простой выборке, так и на более сложной выборке. Но в то же время надо помнить, что у нас было всего три темы, а реальность намного более сурова. Часто мы даже не знаем, сколько у нас тем, и мы не можем так легко воспользоваться, например, K Means. Мы попробовали агломеративную кластеризацию и вовремя догадались использовать K Means. Здесь нас очень выручило то, что мы знали, сколько кластеров должно получиться.

[БЕЗ_ЗВУКА] Ну что ж, мы с вами познакомились с задачей кластеризации, обсудили то, какой разной она бывает в зависимости о того, в каких целях мы ее решаем, и даже обзорно познакомились с некоторыми популярными алгоритмами кластеризации. Кроме того, мы получили некоторый первый опыт решения задачи кластеризации на практике. Но все это до сих пор было немножечко бессистемно, и настал момент задуматься: а когда какие методы лучше использовать? Итак, мы с вами рассмотрели метод K-средних, EM-алгоритм, аггломеративную иерархическую кластеризацию и DBSCAN. При этом в scikit-learn несколько больше классификаторов. Сейчас вы видите на слайде подсвеченными цветом те методы, которые мы разобрали. Здесь стоит лишь прокомментировать, что MiniBatchKMeans — это разновидность KMeans для случая достаточно большой выборки. В этой разновидности алгоритм работает не на всей обучающей выборке, а на случайных подмножествах из нее. И метод Ward — это просто разновидность аггломеративной кластеризации, который особенно хорошо работает с евклидовым расстоянием. В стандартной документации scikit-learn есть такой очень красивый пример работы разных алгоритмов на некоторых модельных выборках. Давайте его разберем. Первая ситуация, которая рассматривается, — это ситуация невыпуклых кластеров, то есть кластеров, для которых их внутренность не принадлежит кластеру. И в качестве кластеров здесь выступают два концентрических кольца. Ну то есть один кластер действительно находится в другом кластере. И как вы видите, далеко не все алгоритмы могут с этой ситуацией справиться. Видно, что с ней справляется только спектральная кластеризация (которую мы не обсуждали), аггломеративная кластеризация и DBSCAN. Другая ситуация, которая рассматривается, — это кластеры-ленты. Здесь тоже не все алгоритмы могут справиться. Но не факт, что вам нужно выделять именно такие кластеры в вашей задаче. Более простая, для того чтобы ее представить, ситуация — это несколько групп точек, которые очень логично разделяются на похожие по форме кластеры. Здесь вы тоже можете заметить, что не все алгоритмы работают удачно, но при этом стоит сказать, что некоторые из алгоритмов имеют в качестве параметров количество кластеров, в частности KMeans и аггломеративная кластеризация. И здесь количество кластеров было заданным 2. Ну и последний случай, который хочется рассмотреть, — это случай равномерного заполнения точками пространства признаков. Конечно, в этом случае мы ожидаем, что адекватный алгоритм кластеризации, наверное, не будет никак делить все на кластеры и скажет, что все это относится в один кластер. Из всех этих алгоритмов мы с вами рассмотрели KMeans, аггломеративную кластеризацию и DBSCAN. И это не случайно. Дело в том, что аггломеративная кластеризация и DBSCAN, как вы видите, дают, пожалуй, одни из самых адекватных результатов. При этом KMeans тоже стоит включить в рассмотрение в силу его простоты и возможности работать с очень большими выборками. Итак, рассмотрим эти методы чуть подробнее. В методе K-средних в качестве параметров выступает только один параметр — число кластеров. При это MiniBatch реализация KMeans может работать на очень большой выборке. Число кластеров при этом должно оставаться средним, кластеры предполагаются выпуклыми и примерно одинаковой формы, и в принципе метод построен из соображения, что расстояние евклидово. Хотя, конечно, вам ничто не мешает использовать какое-то другое расстояние, но это будет не совсем корректно, о чем мы еще поговорим впоследствии. Другой метод, который, кстати, не был проиллюстрирован на предыдущей картинке, — это EM-алгоритм с нормальным распределением в каждом кластере. Ну, имеется в виду многомерное нормальное распределение. Здесь, конечно, параметров очень много: это и веса компонент, и векторы средних, и матрицы ковариаций. И понятно, что для того чтобы восстановить эти параметры, вам нужна достаточно большая выборка — ну чтобы не переобучиться. Но, с другой стороны, если ваша выборка будет очень велика, этот метод не будет очень уже масштабируем. Он даже лучше подходит не для задачи кластеризации, а для задачи восстановления плотности, когда вам нужно не только сгруппировать точки, но и понять, какая плотность в каждой точке пространства признаков. При этом кластеры предполагаются выпуклыми. В некотором смысле можно сказать, что в этом методе возникает некая метрика, которая представляет собой обобщение евклидовой метрики. Мы еще поговорим об этом подробнее в следующем уроке. В аггломеративной кластеризации в качестве параметров нам нужно задать число кластеров; метод, которым вычисляется расстояние между кластерами; и метрику. При этом у нас может быть много примеров и много кластеров. И даже лучше, если кластеров будет много, потому что при небольшом числе кластеров у аггломеративной кластеризации есть некоторая тенденция образовывать один большой кластер и остальные значительно меньше. Кроме того, здесь можно выбирать метрику или функцию близости. Именно это и прельстило нас, когда мы сначала попробовали использовать аггломеративную кластеризацию, кластеризуя тексты по темам. Мы надеялись, что, может быть, косинусная мера приведет к более хорошим результатам, но не учли того, что аггломеративная кластеризация лучше работает, когда кластеров много. И еще один метод — это DBSCAN. В нем нужно задавать радиус окрестности и количество соседей, которое должно быть у точки в этой окрестности, чтобы сказать, что плотность точек в этом месте довольно высока. Он может работать для большого количества объектов, для среднего числа кластеров; может учитывать непохожие друг на друга кластеры, не выпуклые; может обрабатывать выбросы и отсеивать их. И в реализации scikit-learn предполагает, что расстояние евклидово. Хотя в принципе ничто не мешает вам обобщить этот метод на случай какого-то другого расстояния. Подведем итог. Метод K-средних можно использовать тогда, когда вам нужно что-то очень простое, или тогда, когда у вас очень большая выборка. При этом нужно помнить, что кластеры должны быть примерно одинаковыми по размеру. EM-алгоритм с нормальным распределением лучше использовать для восстановления плотности, нежели для кластеризации. Иерархическая аггломеративная кластеризация хороша тем, что неплохо выделяет кластеры, когда их много, и позволяет задать расстояние. А DBSCAN в целом хорошо справляется с многими разными ситуациями, но при этом не может работать со столь большой выборкой, сколь метод K-средних в MiniBatch реализации.