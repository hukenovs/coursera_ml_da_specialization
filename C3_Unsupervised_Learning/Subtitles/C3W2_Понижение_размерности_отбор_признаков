[БЕЗ_ЗВУКА] Мы начинаем неделю, посвященную отбору признаков и понижению размерности, в которой будем говорить о самых разных проявлениях этих задач. Но в первую очередь давайте разберемся, зачем вообще нужно работать с признаками и пытаться их отбирать. Итак, признаков, которые присутствуют в ваших данных, может быть слишком много, больше чем нужно. Из-за чего? Например, вы дали волю фантазии, когда придумывали признаки для вашей задачи, или заказчик отгрузил вам все таблицы, все столбцы из таблиц, которые у него были, и понятно, что там может быть много лишней информации. И при этом, скорее всего, вы можете повысить качество решения задачи, если выберете только важные признаки, только те, которые реально важны для решения задачи. Или, например, сформируете новые признаки на основе старых, так что новых признаков будет меньше, но при этом они будут настолько же информативны, насколько и старые признаки. И давайте начнем с того, что поговорим о том, чем плохи признаки, которые имеют мало отношения к вашей задаче. И, конечно, начнем мы с шумовых признаков — так будем называть те признаки, которые никак не связаны с целевой переменной, которые никак не относятся к той задаче, которую вы решаете. К сожалению, не всегда можно понять по обучающей выборке, что вы имеете дело именно с такими признаками. Давайте рассмотрим простой пример. Представьте, что вы добавляете в выборку 1000 новых признаков, при этом все они абсолютно случайные. Значения каждого признака генерируются из стандартного нормального распределения. Понятно, что эти признаки бесполезны, они никак не помогут решить задачу. Но при этом, поскольку их много, может так оказаться (просто из соображений теории вероятностей), что один из них немножко коррелирует с целевой переменной, при этом, разумеется, он будет коррелировать только на обучающей выборке. На контрольной, поскольку он абсолютно случайный, этой корреляции не будет. При этом модель может подумать, что этот признак важен, и поставит перед ним какой-то вес или как-то еще по-другому учтет его внутри себя. И из-за этого получится, что в модели присутствует шумовой признак, присутствует признак, который случаен. Она зависит от признака, который никак не помогает решать задачу. И из-за этого качество модели, ее обобщающая способность окажутся ниже, чем хотелось бы. Вот еще одна причина. Представьте, что вы используете решающие деревья для решения задачи. И у вас в выборке 1000 признаков, все они информативные. Но при этом, чтобы учесть каждый из них хотя бы один раз, вам понадобится дерево глубины как минимум 10. У этого дерева будет 1000 листьев, и вам понадобится довольно много объектов, чтобы в каждый лист попало достаточное количество примеров, по которым вы сможете построить хорошие прогнозы. При этом обратите внимание: такое дерево учтет каждый признак один раз. Если вы хотите учесть его еще больше, понадобится еще более глубокое дерево, у него будет еще больше листьев, и нужно будет еще больше объектов обучающей выборки. Понятно, что это не всегда хорошо. Еще одна причина, по которой может понадобится отбирать признаки — это ускорение модели. Дело в том, что чем больше у вас признаков, тем более сложная модель получается. А чем более сложная модель у вас получается, тем больше времени ей требуется, для того чтобы построить прогноз. Но при этом иногда прогнозы нужно строить очень быстро. Представьте простой пример: вы решаете задачу рекомендаций товаров на сайте интернет-магазина. И пользователь, например, ищет себе что-то, нажимает на ссылку в поисковой выдаче и начинает переходить на страницу интересного ему товара. На этой странице есть поле, где показываются рекомендации к этому товару, например похожие товары или аксессуары, или что-то еще, которые должна выдавать ваша модель. И она должна выдать их очень быстро, чтобы пользователь не подумал, что страница слишком долго загружается, что сайт сломался, и не ушел к конкуренту. В этом случае вам нужна очень быстрая модель, и один из подходов к ускорению модели — это отбор признаков, построение модели над самыми важными признаками, которых достаточно, чтобы прогнозы были хорошими. Итак, какие подходы есть к отбору признаков? Самый простой — это одномерный подход. Вы оцениваете сходство каждого признака, связь каждого признака с целевой переменной. Например, измеряете корреляцию или как-то еще. Мы будем говорить о том, как это можно делать в следующих видео этого урока. Такой подход — довольно простой, он не учитывает сложные закономерности, тогда как в машинном обучении модели именно учитывают взаимосвязи признаков, взаимное влияние признаков, их пар или даже более сложные взаимодействия на целевую переменную. Такой же подоход считает, что все признаки — независимые. Он не всегда хорош, но иногда подходит, чтобы как-то отранжировать признаки, найти наиболее мощные среди них. Более сложные подходы к отбору признаков могут быть устроены следующим образом: вы перебираете некоторым способом различные подмножества признаков, для каждого такого подмножества обучаете модель, которая использует только эти признаки, и оцениваете ее качество. И дальше выбираете то подмножество, которое дает наилучшее качество. Такие подходы оказываются сложными из-за того, что они все переборные — вам нужно как-то перебирать подмножества признаков. Понятно, что всех подмножеств очень много, поэтому поиск должен быть более направленным. Мы тоже будем говорить о том, как лучше всего устроить такой перебор. Далее, вы можете использовать саму модель, чтобы оценивать важность признаков, чтобы отбирать только самые важные. Например, мы уже изучали Lasso, или L1-регуляризацию, которая позволяет отбирать признаки с помощью линейных моделей. Также похожие методы есть и в решающих деревьях, случайном лесе и градиентном бустинге — об этом мы тоже поговорим. Наконец, дело может быть даже не в отборе признаков. Представьте, что все признаки важны, но их слишком много. В этом случае может помочь понижение размерности, в котором предлагается строить новые признаки на основе старых, причем так, чтобы новых признаков было поменьше, но при этом они содержали в себе максимальное количество информации из исходных признаков. Итак, мы обсудили, что понижение размерности, отбор признаков — это очень важные задачи, которые помогают повысить качество модели, устранить шумовые признаки и ускорить модель. При этом есть отбор признаков, который отбирает наиболее важные признаки из всего множества, и есть понижение размерности, которое строит новые признаки на основе исходных так, чтобы новых признаков было меньше, но информации в них осталось как можно больше. В следующем видео мы поговорим о самых простых методах отбора признаков — одномерных.

[БЕЗ_ЗВУКА] В этом видео мы поговорим о самых простых и наивных методах отбора признаков — одномерных методах. Нам понадобится несколько обозначений. Через xij будем обозначать значение j‐того признака на i‐том объекте. Через xj с верхней чертой — среднее значение j‐того признака по всей выборке. yi — значение целевой переменной или ответа на i‐том объекте. y с верхней чертой — среднее значение целевой переменной на всей выборке. Наша задача — оценить предсказательную силу или информативность каждого признака. То есть то, насколько хорошо по данному признаку можно предсказывать целевую переменную. Далее данные оцененной информативности можно использовать, например, чтобы отобрать k лучших признаков, k признаков с наибольшей информативностью. Или отобрать те признаки, у которых значение информативности больше порога, например больше некоторой квантили распределения информативности. Один из самых простых методов, который можно использовать, чтобы измерить связь между переменной, то есть признаком, и ответами, — это корреляция. Вы уже хорошо с ней знакомы и знаете, что она вычисляется по вот такой непростой формуле. Чем больше по модулю корреляция между признаком и целевой переменной, тем мы будем считать более информативным данный признак. При этом у корреляции, вспомним, есть одно интересное свойство: она максимальна по модулю, то есть равна +1 или −1, если между признаком и целевой переменной есть линейная связь. То есть если целевую переменную можно строго линейно выразить через значение признака. Это означает, что корреляция измеряет только линейную информативность, то есть способность признака линейно предсказывать целевую переменную. Обратим внимание, что корреляция, вообще говоря, расчитана на вещественные признаки и вещественные ответы. Тем не менее, её можно использовать в случае, если признаки и ответы бинарные. В этом случае имеет смысл кодировать бинарный признак или ответ с помощью значений −1 и +1. Рассмотрим немножко другую постановку задачи. Предположим, мы решаем задачу бинарной классификации и хотим оценить важность j‐того признака для решения именно этой задачи бинарной классификации. В этом случае мы можем попробовать построить классификатор, который использует лишь этот один j‐тый признак, и оценить его качество. Например, можно рассмотреть очень простой классификатор, который берёт значение j‐того признака на объекте, сравнивает его с порогом t. И если значение меньше этого порога — то он относит объект к первому классу, если же меньше порога — то к отрицательному классу: нулевому или минус первому, в зависимости от того, как мы его обозначили. Далее, поскольку этот классификатор зависит от параметра t, от порога t, то его качество можно измерить с помощью метрики вроде площади под ROC-кривой или площади под Precision-Recall кривой в зависимости от того, что важнее в данной задаче. Далее можно по данной площади отсортировать все признаки и выбирать лучшие. Есть и ещё один подход в одномерном оценивании качества признаков, который основан на метриках из теории информации. При этом совершенно не нужно знать теорию информации, чтобы использовать их. Примером такой метрики является взаимная информация, или mutual information. Давайте введём несколько обозначений, чтобы записать данную метрику. Она рассчитана на ситуацию, в которой и признак, и целевая переменная являются дискретными, то есть принимают конечное число значений. Будем считать, что у нас задача многоклассовой классификации. В этом случае целевая переменная принимает m разных значений, которые будем обозначать как 1, 2,..., m. И будем считать, что признак тоже принимает лишь n значений, которые обозначим как 1, 2,..., n. Поскольку для метрики взаимной информации не важны конкретные значения признаков, можно обозначать их вот так с помощью натуральных чисел. Вероятностью некоторого события будем обозначать долю объектов, для которых это событие выполнено. Например, вероятность того, что признак, значение признака равно v, а значение целевой переменной равно k, или P (x = v, y = k), будем вычислять как долю объектов, у которых значение признака равно v и одновременно значение целевой переменной равно k. Или, например, вероятность того, что x = v будем вычислять как долю объектов выборки, на которых значение j-того признака равняется v. Используя эти обозначения, введём взаимную информацию. Взаимная информация между j-тым признаком и целевой переменой вычисляется вот по этой сложной формуле. Она зависит как раз от вероятности того, что j-тый признак равен v, целевая переменная равна k одновременно, и от маргинальных вероятностей, то есть отдельных вероятностей событий, что признак, j-тый признак равен v и целевая переменная равна k. Главная особенность взаимной информации состоит в следующем. Предположим, что события «признак принимает значение v» и «целевая переменная принимает значение k» независимы, то есть значение признака и целевой переменной никак не связаны между собой. В этом случае совместная вероятность, которая стоит в числителе под логарифмом, распадётся на две маргинальные вероятности: Px = v и Py = k. Тогда числитель и знаменатель сократятся, и под логарифмом окажется единица. Логарифм единицы равен нулю, и взаимная информация равна нулю. То есть взаимная информация будет равна нулю, если признак и целевая переменная независимы. Если же между ними есть какая-то связь, то взаимная информация будет отличаться от нуля. Причём она может быть как больше, так и меньше нуля. Это означает, что информативность признаков нужно оценивать по модулю взаимной информации. Обсудим некоторые проблемы подхода, в котором мы оцениваем важности всех признаков по отдельности. Вот простой пример двумерной выборки, для которой нужно решить задачу классификации. Заметьте, если спроецировать данную выборку на ось абсцисс, на ось x, то она будет как-то разделима. Да, там будут ошибки, но более-менее класс разделить можно. Если же спроецировать данную выборку на ось ординат, на ось y, то все объекты разных классов перемешаются, и выборка будет практически неразделима. В этом случае любой метод одномерного оценивания информативности признаков скажет, что первый признак более-менее информативен, второй признак — совершенно не информативен. Тем не менее, если использовать эти признаки одновременно, то эти классы будут разделимы идеально. То есть, на самом деле, второй признак важен, но он важен только в совокупности с первым признаком, и методы, которые мы только что обсудили, не способны это обнаружить. А вот ещё более тяжёлый пример. В этом случае, если спроецировать выборку на ось x или на ось y, то объекты классов перемешаются, и в обоих случаях данные будут совершенно неразделимы, задачи будут нерешаемые. И опять же, любой из методов, которые мы обсудили, скажет, что оба признака совершенно не информативны. Тем не менее, если использовать их одновременно, то, например, решающее дерево может идеально решить данную задачу классификации. Итак, мы обсудили разные подходы к одномерному отбору признаков. Например, измерение корреляции, измерение взаимной информации или измерение площади под кривой для простого классификатора, который просто выбирает класс, отсекая по порогу значения признака. Тем не менее, у этих подходов есть некоторые проблемы. Они не учитывают взаимосвязь признаков, зависимость целевой переменной от сложных комбинаций признаков. В следующем видео мы поговорим о методах, которые позволяют учитывать такие взаимосвязи при отборах признаков, о жадных методах отбора признаков.

[ОТСУТСТВУЕТ_ЗВУК] В этом видео мы поговорим о жадных методах отбора признаков, которые позволяют отбирать оптимальные признаки для произвольной модели машинного обучения. Итак, мы уже в первом видео этого урока упоминали такие методы. Они, по сути, являются надстройками над методами обучения моделей. То есть они перебирают каким-то образом различные подмножества признаков, для каждого подмножества строят модель, оценивают ее качество и выбирают то подмножество, которое дает наилучшее качество этой модели машинного обучения. Как устроен этот процесс? Мы считаем обучение модели черным ящиком, который на вход принимает информацию о том, как какие из его признаков можно использовать при обучении модели, обучает модель, и дальше как-то мы оцениваем ее качество. Например, по отложенной выборке, кросс-валидации или как-то еще. По сути, здесь должна решаться задача оптимизации этого черного ящика по подмножеству признаков. Подмножество признаков — это дискретный объект, и мы приходим к задаче дискретной оптимизации, которые, как правило, очень сложные. И вряд ли можно придумать что-то лучше, чем полный перебор всех возможных вариантов, то есть всех подмножеств признаков. Собственно, так вот подходим к первому способу отбора признаков. Полный перебор. Мы пробуем все возможные подмножества признаков и оцениваем их качество. Выбираем то, которое оказывается лучше всего. Можно немного структурировать перебор и перебирать подмножества последовательно. Сначала те, которые имеют мощность 1, то есть наборы из 1 признака, потом наборы из 2 признаков, наборы из 3 и так далее. Это подход очень хороший, он найдет оптимальное подмножество признаков, но при этом очень сложный, поскольку всего таких подмножеств 2 в степени d, где d — это число признаков. Если признаков много — сотни или тысячи, то такой перебор невозможен. Он займет слишком много времени, возможно даже за вашу жизнь вы не успеете узнать правильный ответ. Поэтому такой метод подходит только для небольшого количества признаков. Если их 10, то вы вполне можете найти оптимальное подмножество. Или, например, он может быть неплох, если вы знаете, что информативных признаков, признаков, которые важны для решения задачи, очень мало, единицы. Если же признаков много и вы знаете, что информативных признаков тоже много, то нужна какая-то жадная стратегия. Жадная стратегия используется всегда, когда полный перебор не подходит для решения той или иной задачи. Например, может оказаться неплохой стратегия жадного наращивания, жадного добавления. Сначала мы находим один признак, который дает наилучшее качество модели. То есть такой один признак, что если над ним построить модель, она получится лучшей среди всех моделей над одним признаком. Обозначим его как i1, это его индекс, и обозначим множество, состоящее из этого одного признака, как J1. Дальше попытаемся добавить к этому множеству еще один признак так, чтобы как можно сильнее уменьшить ошибку модели. То есть мы будем перебирать все признаки i, добавлять их к признаку i1, строить над этим модели и оценивать их ошибку. Функция Q как раз оценивает ошибку модели, построенной на подмножестве признаков, которые передаются ей в качестве аргументов. Мы находим признак i, который дает наименьшее значение ошибки, обозначаем его как i2 и добавляем к нашему подмножеству признаков J. Теперь оно будет обозначаться как J2, поскольку в нем 2 признака. И так далее будем продолжать эту процедуру, добавляя по одному признаку к нашим множествам. Будем получать множество J3, J4 и так далее. Продолжаем это до тех пор, пока ошибка уменьшается. Если в какой-то момент мы не сможем добавить новый признак так, чтобы уменьшить ошибку, мы остановим процедуру. Обратите внимание, что в этом случае, как только мы добавили признак на какой-то итерации, он навсегда остается в подмножестве признаков, мы не можем его удалить. Именно в этом и заключается жадность процедуры. Этот подход довольно быстрый. В нем столько итераций, сколько признаков в выборке. Но при этом он слишком жадный. Если мы добавили признак, мы уже не можем его удалить. Мы перебираем слишком мало вариантов, и хочется как-то усложнить эту процедуру. Один из подходов к усложнению — это алгоритм ADD-DEL, который не только добавляет, но и удаляет признаки из нашего оптимального множества. Давайте обсудим, как он работает. Начинаем мы с того, что проводим процедуру жадного добавления. Наращиваем множество признаков до тех пор, пока получается уменьшить ошибку. Когда мы остановились, мы начинаем жадно удалять признаки из нашего подмножества. То есть мы перебираем все возможные варианты удаления признака, оцениваем ошибку и удаляем тот, который приводит к наибольшему уменьшению ошибки на выборке. Повторяем такую процедуру удаления до тех пор, пока уменьшается ошибка. Как только мы дошли до момента, когда мы не можем уменьшить ошибку удалением признака, снова начинаем добавлять их и так далее. То есть эта процедура повторяет добавление и удаление признаков до тех пор, пока уменьшается ошибка. Этот алгоритм всё еще жадный, но при этом он менее жадный, чем предыдущий, посколько он может исправлять ошибки, сделанные в начале перебора. Если мы вначале добавили какой-то признак, который был не очень хороший, то на этапе удаления можем его устранить и добавить какой-то более подходящий для решения этой задачи. Итак, мы обсудили подход к отбору признаков, который оптимизирует качество произвольной модели. И алгоритм обучения модели мы рассматриваем как некий черный ящик, который принимает на вход только подмножества признаков. Это очень прямой подход к решению задачи отбора признаков, но при этом он приводит к задаче дискретной оптимизации, которая сложная и решать ее можно либо перебором, что может быть слишком трудозатратно, либо жадными алгоритмами. Например, жадным добавлением или алгоритмом ADD-DEL. А в следующем видео мы поговорим о том, как отбирать признаки с помощью моделей машинного обучения.

[ЗАСТАВКА] В этом видео мы поговорим о том, как использовать обученные модели для отбора признаков, для оценивания их информативности. Начнем с линейных моделей. Линейная модель вычисляет взвешенную сумму значений признаков на объекте. При этом мы возвращаем саму взвешенную сумму, если это задача регрессии, и знак этой суммы, если это задача классификации. Если признаки масштабированы, если у них одинаковый масштаб, то веса при признаках можно интерпретировать как информативности: чем больше по модулю вес при j-том признаке, тем больший вклад j-тый признак вносит в ответ модели. При этом если признаки не масштабированы, то так использовать веса уже нельзя. Например, если есть два признака — первый и второй, и при этом второй в тысячу раз больше по масштабу, чем первый, то у первого признака может быть очень большой вес только потому, что нужно выровнять масштабы, что нужно сделать первый признак таким же по масштабу, как и второй. Если есть желание занулить как можно больше весов, чтобы линейная модель учитывала только те признаки, которые наиболее важны для нее, то можно использовать L1-регуляризацию. Чем больше будет коэффициент при L1-регуляризаторе, тем больше весов будет занулено, тем меньше признаков будет использовать линейная модель. Еще один вид моделей, которые мы обсуждали в предыдущем курсе — это решающие деревья. Напомню, что решающие деревья мы строим «жадно» — растим их от корня к листьям, и при этом на каждом этапе мы пытаемся разбить вершину на две. Чтобы разбить вершину, нужно выбрать признак, по которому мы будем разбивать, и порог, с которым мы будем сравнивать значение данного признака. Если значение признака меньше этого порога, то отправляем объект в левое поддерево, если больше этого порога, то — в правое поддерево. Выбор признака и порога осуществляется по вот такому критерию Q, который вычисляет взвешенную сумму критериев информативности в обеих дочерних вершинах — левой и правой. Чем меньше данная взвешенная сумма, тем больше j-тый признак и порог t подходят для разбиения. Мы рассматривали самые разные варианты критериев информативности. Например, в регрессии это может быть среднеквадратичная ошибка, в классификации это может быть критерий Джини или энтропийный критерий. Обратим внимание, что если мы в данной вершине сделали разбиение по признаку j, то чем сильнее уменьшили значение критерия информативности, тем лучше данный признак, тем важнее он оказался при построении дерева. Таким образом, можем оценивать важность признака на основе того, насколько сильно он смог уменьшить значение критерия информативности. Допустим, в вершине m мы произвели разбиение по j-тому признаку. Тогда вычислим в ней уменьшение критерия информативности по вот такой формуле: разность его значения в вершине Xm и в дочерних вершинах Xl и Xr. Далее просуммируем данное уменьшение по всем вершинам дерева, в которых разбиение делалось по j-тому признаку. Обозначим эту сумму как Rj, и чем больше Rj, тем важнее данный признак был при построении дерева. Сами по себе решающие деревья не очень полезны, но они очень активно используются при построении композиций, в частности, в случайных лесах и в градиентном бустинге над деревьями. В данных композициях измерить важность признака можно аналогично. Просто просуммируем уменьшение критерия информативности Rj по всем деревьям композиции, и чем больше данная сумма, тем важнее j-тый признак для композиции. То есть, по сути, мы оцениваем признаки с помощью того, насколько сильно они смогли уменьшить значение критерия информативности в совокупности по всем деревьям композиции. Для случайного леса можно предложить еще один интересный способ оценивания информативности признаков. Вспомним, что в случайном лесе каждое базовое дерево bn обучается по подмножеству объектов обучающей выборки — по подвыборке. Таким образом, есть объекты, на которых дерево не обучалось, и, по сути, набор этих объектов является валидационной выборкой для n-го дерева. Такая выборка называется out-of-bag. Итак, метод заключается в следующем: возьмем n-ое базовое дерево bn. Оценим ошибку данного дерева Qn по out-of-bag-выборке, то есть по тем объектам, которые не использовались для обучения, и запомним ее. После этого перемешаем значения j-того признака, то есть возьмем матрицу «объекты-признаки», возьмем j-тый столбец в этой матрице, то есть все значения j-того признака, и случайным образом переставим значения в этом столбце, то есть превратим j-тый признак в абсолютно бесполезный шумовой признак. После этого применим то же самое дерево bn к данной выборке с перемешанным j-тым признаком и оценим качество дерева на этой выборке — на out-of-bag-подвыборке. Обозначим ошибку на out-of-bag-подвыборке через Qn'. По сути, данная ошибка Qn' обозначает то... она будет тем больше, чем сильнее дерево использует j-тый признак. Если j-тый признак активно используется в дереве, то понятно, что ошибка сильно уменьшится, поскольку мы испортили значение данного признака. Если же данный признак совершенно не важен для дерева и не используется в нем, то ошибка практически не изменится. Таким образом информативность j-того признака будем оценивать как разность Qn', то есть ошибки на испорченной выборке, и Qn, то есть ошибки на исходной выборке, где j-тый признак имеет те значения, которые должен иметь. Чем больше данная разность, тем важнее признак для n-го дерева. Дальше мы усредним эти информативности по всем деревьям случайного леса, и, естественно, чем больше будет данное среднее значение, тем информативнее признак. На практике оказывается, что информативности, вычисленные вот таким образом, и информативности, вычисленные как сумма уменьшения критерия информативности, которые мы обсуждали раньше, оказываются очень связаны между собой, они очень похожи. Итак, мы обсудили, как отбирать признаки с помощью разных моделей: с помощью линейных — в этом случае нужно использовать веса при масштабированных признаках, с помощью деревьев — в этом случае можно использовать уменьшение критерия информативности, и этот же подход работает в случайных лесах и в градиентном бустинге над деревьями. И также обсудили один подход для случайных лесов, который использует перемешивание значений признака. В следующем видео мы перейдем от отбора признаков к более общей задаче — к понижению размерности — и подробно обсудим эту постановку.

В этом видео мы начнем говорить о задаче понижения размерности. О том, зачем она нужна, чем отличается от отбора признаков, и обсудим один простой подход к ее решению. Для начала давайте рассмотрим пару примеров. Вот есть такая выборка, у нее три размерности. При этом видно, что если просто убрать из нее признак, который отложен по оси Z, то мы получим двумерную выборку, в которой классы будут все еще отлично разделяться между собой. Синий, зеленый и красный будут разделимы даже линейными методами. А вот более сложный случай. Здесь кажется, что оба признака — значимые. Но при этом можно заметить, что можно спроецировать эти данные на прямую, сделав их одномерными, не потеряв при этом практически никакой информации. То есть да, оба признака значимые, но при этом они линейно зависимые. И этим можно воспользоваться, чтобы устранить некоторую избыточность в данных. Но при этом отбора признаков здесь не хватит. Нужно сформировать новый признак на основе двух исходных. Бывают и еще более сложные случаи, как, например, здесь. Здесь тоже видно, что можно спроецировать выборку на некоторую кривую, но при этом кривая очень нелинейная, и скорее всего, найти ее будет довольно сложно. Так мы приходим к задаче понижения размерности, которая состоит в формировании новых признаков на основе исходных. При этом новых признаков должно быть меньше, чем исходных, но они должны сохранять в себе как можно больше информации, которая содержалась в исходных признаках. Один из основных подходов к понижению размерности — это линейный подход, в котором каждый новый признак представляет собой линейную комбинацию исходных признаков. Если говорить более формально, то будем считать, что исходных признаков у нас D штук, при этом значение j-того исходного признака на i-том объекте выборки будем обозначать, как xᵢⱼ. А новых признаков при этом d штук, и значение j-того нового признака на i-том объекте выборки обозначаем, как zᵢⱼ. И будем считать, что раз у нас подход линейный, то признак zᵢⱼ — новый j-тый признак на i-том объекте — линейно выражается через исходные признаки на этом же i-том объекте, xᵢⱼ. И при этом они складываются с весами wⱼk-тое, где вес wⱼk-тое показывает, какой именно вклад k-тый исходный признак делает в j-тый новый признак на каждом объекте. Один из простейших подходов к такому понижению размерности — это метод случайных проекций, в котором мы просто случайно генерируем все эти веса wⱼk-тое, например, из нормального распределения. То есть wⱼk-тое мы просто выбираем из нормального распределения с центром в 0 и дисперсией, например, 1/d, где d — это количество новых признаков, которое мы хотим сделать. Этот подход не такой уж необоснованный, у него есть некоторое обоснование в виде леммы Джонсона-Линденштраусса, которая говорит следующее: если у нас есть выборка, в которой объектов меньше, чем признаков, то есть это выборка в очень многомерном признаковом пространстве, то ее можно спроецировать в пространство меньшей размерности так, что при этом расстояния между объектами практически не изменятся. То есть мы сохраним топологию в этом признаковом пространстве. При этом, если мы хотим, чтобы расстояния изменились не больше, чем на ε, то нужно брать размерность нового признакового пространства (количество новых признаков d) не меньше, чем вот такая дробь: 8 * ln количества объектов в выборке / ε². Да, эта лемма лишь говорит о том, что существует такая проекция, что она будет сохранять расстояния между парами признаков. Но при этом на практике оказывается, что если взять d, соответствующее вот такой формуле, то даже метод случайных проекций строит такое преобразование признаков, так понижает размерность выборки, что расстояния сохраняются неплохо, и это понижение оказывается довольно качественным. Такой подход оказывается хорошо работающим для текстов, где размерности всегда оказываются большими. Мы кодируем каждый текст с помощью признаков, с помощью мешка слов, и при этом признаков у нас столько, сколько слов в словаре, и поэтому признаков оказывается очень много. И даже метод случайных подпространств работает очень хорошо на таких выборках, хотя, конечно, есть и более интеллектуальные подходы. Итак: мы начали разговор о методах понижения размерности, поговорили, зачем это нужно, и что это гораздо более общий подход, чем отбор признаков, и обсудили один очень простой подход — метод случайных подпространств. А в следующем видео мы поговорим о более продвинутом подходе к понижению размерности — методе главных компонент.

В этом видео мы начнем разговор о методе главных компонент, который является одним из наиболее популярных методов при решении задачи понижения размерности. В прошлый раз мы начали говорить о линейном подходе к понижению размерности, в котором каждый новый признак линейно выражается через исходные. И выводили следующие обозначения: xi-тая j-тая — это значение исходного j-того признака на i-том объекте. И всего таких исходных признаков D(большое) штук. zi-тая j-тая — это значение нового j-того признака на i-том объекте. И всего таких новых преобразованных признаков d (маленькое) штук. В линейном подходе zi-тая j-тая, то есть значение j-того признака на i-том объекте линейно выражается через исходные признаки. То есть мы суммируем исходные признаки xi-тая j-тая с весами wj-тая k-тая. На самом деле эту формулу можно переписать в матричном виде. Для этого немного поработаем над ней. Поменяем местами множители wj-тая k-тая и xi-тая j-тая, что всегда можно сделать без потерь в этой формуле, и попробуем привести это к формуле умножения матриц. Для этого перейдем от матрицы wj-тая k-тая к транспонированной. Тогда индексы будут k и j, w транспонированная k-тая j-тая. А это уже формула перемножения матриц x и w. То есть получается, что матрица новых признаков Z вычисляется как произведение исходной матрицы объекты-признаки X и транспонированной матрицы W. Потребуем, чтобы матрица весов W была ортогональной. То есть чтобы произведение W транспонированного на W равнялась единичной матрице. На самом деле это ограничение никак не будет нам мешать. Если б мы начали решать задачу без него, то оказалось бы, что нам нужно ввести некоторое подобное ограничение, чтобы решение было единственным. Итак, если данное требование будет выполнено, то из формулы Z = X * W транспонированное можно получить формулу для X, поскольку матрица W ортогональна. X будет вычисляться как Z * W. Понятно, что это равенство вряд ли можно выполнить точно. Поскольку в матрице Z будет меньше признаков, чем в X, то если ранг матрицы X больше, чем d, больше, чем число новых признаков, то данное равенство точно нельзя будет выполнить строго. В этом случае будем требовать, чтобы отклонение исходной матрицы признаков X от восстановленной матрицы X с помощью произведения Z на W было как можно меньше, чтобы эти матрицы были как можно сильнее похожи друг на друга. Размер этого отклонения будем вычислять с помощью нормы Фробениуса разности матриц X и Z * W. Норма Фробениуса матрицы — это просто сумма квадратов ее значений, аналог l2 норма для вектора. Будем минимизировать данную норму. По сути, это задача матричного разложения. Мы хотим представить матрицу X в виде произведения двух матриц Z и W, которые будут иметь меньший ранг. То есть мы хотим уменьшить ранг матрицы, при этом потеряв как можно меньше информации в ней. Есть и немного другой подход к постановке задачи метода главных компонент. Представьте, что у нас есть вот такая выборка, и мы хотим спроецировать ее на некоторую прямую. В этом случае прямая будет тем лучше, чем меньше будет ошибка проецирования. Под ошибкой проецирования мы понимаем сумму по всей выборке расстояний от объекта до его проекции на эту прямую. Понятно, что чем меньше эти расстояния, тем лучше прямая приближает данные, тем меньше будет ошибка и тем больше информации мы сохраним. В идеальном случае мы хотели бы провести прямую так, чтобы она уже проходила через все объекты выборки. Но понятно, что в данном случае это невозможно. Поэтому будем искать такую прямую, расстояние от которой до всех объектов в сумме будет минимальным. В общем случае, когда признаков много, мы будем пытаться проецировать выборку не на плоскость, не на прямую, а на гиперплоскость. Можно показать, это известно из аналитической геометрии, что есть два способа задания гиперплоскости. Первый — с помощью вектора нормали. Мы его использовали, например, в линейных методах. Второй — с помощью направляющих векторов. Если размерность пространства D, и мы строим в нем гиперплоскость, то если выбрать на этой плоскости D линейно независимых векторов, которые лежат в ней, то они тоже будут однозначно задавать эту плоскость, эту гиперплоскость. И если эти векторы, направляющие векторы, составить в матрицу W, так что каждый столбец этой матрицы, это один направляющий вектор, то опять же можно показать, что проекция точки xi-тая на данную гиперплоскость будет вычисляться по формуле xi-тая * W. Тогда мы будем, чтобы минимизировать ошибку проецирования на гиперплоскость, мы будем минимизировать сумму норм отклонений xi-тых исходных объектов, от их проекции xi-тая на W. На самом деле у этой формулы есть некоторые проблемы. xi-тая на w — это вектор немного другой размерности, который задает... задается координатами в проекционных векторах. Эту проблему можно решить, если перейти немножко к другому базису, но не будем об этом. Нам сейчас важна идея. Можно решать задачу путем минимизации отклонений объектов от проекционной гиперплоскости. Есть и третий взгляд на метод главных компонент. Представьте, что у нас есть вот такая выборка. И мы хотим снова выбрать прямую, на которую будет оптимально ее спроецировать. Понятно, что синяя прямая будет лучше, поскольку при проецировании на нее сохранится гораздо больше информации выборки. Как формализовать понятие информации? Например, можно говорить о дисперсии. Чем больше будет дисперсия выборки после проецирования на прямую, тем лучше, тем больше информации мы сохранили. Собственно, для данного случая этот критерий хорошо подходит. Дисперсия выборки после проецирования на синюю прямую будет гораздо больше, чем после проецирования на красную прямую. Формально дисперсию выборки после проецирования можно записать с помощью вот такой формулы, которая представляет собой сумму по всем новым признакам, произведение столбцов матриц весов wj-тое и матриц объекты-признаки для исходного набора признаков. Чем больше значение этой суммы, тем больше дисперсия выборки после проецирования на гиперплоскость, которая задается матрицей весов w. Таким образом, данное выражение нужно максимизировать. Чем оно будет больше, тем больше информации выборки мы сохраним после проецирования. Итак, мы обсудили три постановки задачи. Первая основана на матричном разложении исходной матрицы объекты-признаки и x. Вторая — на поиске оптимальной проекционной гиперплоскости, третья — на поиске такой проекционной гиперплоскости, при которой получается максимальная дисперсия выборки после проецирования. Оказывается, что все эти постановки задач эквивалентны, они все приводят к одному и тому же решению, и поэтому они все являются постановками задач метода главных компонент. А о том, как именно решаются эти задачи, мы поговорим уже в следующем видео.

[БЕЗ_ЗВУКА] В прошлом видео вы узнали, как формулируется задача метода главных компонент, а теперь давайте разберемся, как ее решать. Как вы знаете, одна из постановок метода главных компонент — это максимизация дисперсии. Есть некоторое облако точек, где каждая точка — это объект выборки, и нужно найти такие оси, при проецировании на которые мы сохраним как можно больше дисперсии исходной выборки. Чем больше дисперсии мы сохраним, тем больше информации останется после понижения размерности. Формально задача максимизации дисперсии после проецирования записывается вот так. В первой строке записана собственно дисперсия после проецирования — она выражается через направление wi, а поскольку мы хотим оставить как можно больше информации, дисперсию нужно максимизировать. Также мы вводим органичение, что матрица весов W должна быть ортогональной — это нужно, чтобы решение было единственным. В методе главных компонент есть один нюанс: выражение, через которое мы записали дисперсию, будет означать именно дисперсию выборки только в том случае, если матрица объекты-признаки центрирована, то есть если среднее каждого признака равно нулю. Поэтому будем считать, что, собственно, выборка центрирована, что мы уже вычли среднее из каждого столбца в матрице объекты-признаки. Итак, чтобы разобраться, как устроено решение этой задачи, давайте попробуем сначала разобраться с простым частным случаем — случаем, когда мы хотим найти ровно одну компоненту, на которую будем проецировать всю выборку, и хотим выбрать ее так, чтобы дисперсия после проецирования была максимальной. Задача будет выглядеть вот так. Вектор весов, который характеризует направление — это w1, и он умножается на матрицу X транспонированное, это умножается на матрицу X и снова на вектор весов w1. Это мы хотим максимизировать. Ограничение же вырождается в то, что L2 норма этого вектора весов должна равняться единице. Итак, как вы знаете, чтобы решать такие задачи условной оптимизации, нужно выписать их лагранжиан. Лагранжиан будет выглядеть вот так. Это собственно критерий, который мы максимизируем минус λ умножить на ограничение. λ умножить на w1 транспонированное на w1 минус единица. Далее этот лагранжиан нужно продифференцировать по тому, что мы хотим найти, то есть по w1. Если воспользоваться формулами матричного дифференцирования, можно получить вот такую формулу. Если там ее немного преобразовать, перенести одно слагаемое в правую часть и поделить на 2, то мы получим вот такое уравнение. X транспонированное на X умножить на w1 равняется λ на w1. Давайте обратим внимание, что что говорит это уравнение? Оно говорит, что вектор w1 является собственным вектором матрицы X транспонированное на X, поскольку при умножении матрицы на этот вектор, мы получаем этот же вектор, умноженный на некоторое число λ, и число λ является собственным значением, соответствующим этому собственному вектору. Если мы подставим полученное нами условие в функционал задачи, то обнаружим, что дисперсия выборки после проецирования будет равна как раз λ, то есть будет равна собственному значению, соответствующему собственному вектору, который мы выбрали. Таким образом, поскольку мы хотим максимизировать дисперсию, нам нужно выбирать максимальное собственное значение и собственный вектор, который соответствует этому максимальному собственному значению. Итак, мы получаем, что первая компонента в методе главных компонент — это собственный вектор матрицы X транспонированное на X, который соответствует максимальному собственному значению этой матрицы. Кстати, обратите внимание, что X транспонированное на X — это матрица ковариации, то есть именно та матрица, которая характеризует дисперсию нашей выборки. Неудивительно, что именно через ее характеристики выражается решение метода главных компонент. Визуально это выглядит вот так: есть облако точек, и мы выбираем именно то направление, при проецировании на которое мы сохраним как можно больше дисперсии. И, собственно, это направление будет как раз равно первому собственному вектору матрицы ковариации. Если мы продолжим наши выкладки и будем искать оптимальное второе направление, третье и так далее, то обнаружим, что, например, w2 — второе направление в методе главных компонент — соответствует собственному вектору матрицы X транспонированное на X, соответствующему второму по величине собственному значению, и так далее. Кстати, отсюда же можно получить, что через собственные значения можно выразить долю дисперсии, которую мы сохранили. Дисперсия всей исходной выборки — это сумма всех собственных значений матрицы X транспонированное на X, то есть сумма от 1 до D; дисперсия выборки после проецирования на d главных компонент — это сумма максимальных d собственных значений. Таким образом, с помощью вот такой дроби можно понять, какую долю дисперсии мы сохранили, спроецировав нашу выборку на главные компоненты. При решении задачи метода главных компонент очень пригождается сингулярное разложение, про которое вы уже слушали в первом курсе. Напомню, сингулярное разложение матрицы X представляет эту матрицу в виде произведения трех других матриц: U умножить на D умножить на V транспонированное. При этом U и V — это ортогональные матрицы, а D — диагональная матрица. Столбцы матрицы U — это собственные векторы матрицы X на X транспонированное, столбцы матрицы V — это собственные векторы матрицы X транспонированное на X, а на диагонали матрицы D стоят собственные значения этих обоих матриц, которые, оказывается, совпадают. Кстати, эти собственные значения матриц X транспонированное на X и X на X транспонированное называются сингулярными числами. Итак, мы приходим к следующему алгоритму для решения задачи метода главных компонент. Мы находим сингулярное разложение матрицы X, формируем матрицу весов W из собственных векторов, из столбцов матрицы V, соответствующих максимальным сингулярным числам, и после этого можем делать преобразования. Чтобы спроецировать объекты, записанные в матрицу X на наши главные компоненты, мы просто умножаем матрицу X на W и получаем матрицу Z, которая является матрицей объекты-признаки для нового сокращенного признакового описания. Итак, мы с вами разобрались, как решать задачу метода главных компонент. Чтобы ее решить, нужно найти собственные векторы матрицы ковариации X транспонированное на X и выбрать те из них, которые соответствуют максимальным собственным значениям. Именно проецирование на эти векторы будет позволять сохранить как можно больше дисперсию. Также мы выяснили, что через собственные значения этой матрицы можно выяснить, какую именно долю дисперсии мы сохранили при таком понижении размерности.